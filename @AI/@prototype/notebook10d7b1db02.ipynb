{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Intro"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This dataset contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. For a detailed description of the dataset and how it was compiled please refer to our paper.\n","All excerpts are taken from field recordings uploaded to www.freesound.org. The files are pre-sorted into ten folds (folders named fold1-fold10) to help in the reproduction of and comparison with the automatic classification results reported in the article above.\n","\n","In addition to the sound excerpts, a CSV file containing metadata about each excerpt is also provided."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Methodology"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["1. There are 3 basic methods to extract features from audio file :\n","    a) Using the mffcs data of the audio files\n","    b) Using a spectogram image of the audio and then converting the same to data points (As is done for images). This is easily done using mel_spectogram function of Librosa\n","    c) Combining both features to build a better model. (Requires a lot of time to read and extract data).\n","2. I have chosen to use the second method.\n","3. The labels have been converted to categorical data for classification.\n","4. CNN has been used as the primary layer to classify data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Importing Necessary Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Basic Libraries\n","\n","import pandas as pd\n","import numpy as np\n","\n","pd.plotting.register_matplotlib_converters()\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import GridSearchCV\n","\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Libraries for Classification and building Models\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Dropout\n","from tensorflow.keras.utils import to_categorical \n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Project Specific Libraries\n","\n","import os\n","import librosa\n","import librosa.display\n","import glob \n","import skimage"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Analysing Data Type and Format"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Analysing CSV Data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"../input/urbansound8k/UrbanSound8K.csv\")\n","\n","'''We will extract classes from this metadata.'''\n","\n","df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Column Names\n","\n","* slice_file_name: \n","The name of the audio file. The name takes the following format: [fsID]-[classID]-[occurrenceID]-[sliceID].wav, where:\n","[fsID] = the Freesound ID of the recording from which this excerpt (slice) is taken\n","[classID] = a numeric identifier of the sound class (see description of classID below for further details)\n","[occurrenceID] = a numeric identifier to distinguish different occurrences of the sound within the original recording\n","[sliceID] = a numeric identifier to distinguish different slices taken from the same occurrence\n","\n","* fsID:\n","The Freesound ID of the recording from which this excerpt (slice) is taken\n","\n","* start\n","The start time of the slice in the original Freesound recording\n","\n","* end:\n","The end time of slice in the original Freesound recording\n","\n","* salience:\n","A (subjective) salience rating of the sound. 1 = foreground, 2 = background.\n","\n","* fold:\n","The fold number (1-10) to which this file has been allocated.\n","\n","* classID:\n","A numeric identifier of the sound class:\n","0 = air_conditioner\n","1 = car_horn\n","2 = children_playing\n","3 = dog_bark\n","4 = drilling\n","5 = engine_idling\n","6 = gun_shot\n","7 = jackhammer\n","8 = siren\n","9 = street_music\n","\n","* class:\n","The class name: air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, \n","siren, street_music."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Using Librosa to analyse random sound sample - SPECTOGRAM"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dat1, sampling_rate1 = librosa.load('../input/urbansound8k/fold5/100032-3-0-0.wav')\n","dat2, sampling_rate2 = librosa.load('../input/urbansound8k/fold5/100263-2-0-117.wav')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20, 10))\n","D = librosa.amplitude_to_db(np.abs(librosa.stft(dat1)), ref=np.max)\n","plt.subplot(4, 2, 1)\n","librosa.display.specshow(D, y_axis='linear')\n","plt.colorbar(format='%+2.0f dB')\n","plt.title('Linear-frequency power spectrogram')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20, 10))\n","D = librosa.amplitude_to_db(np.abs(librosa.stft(dat2)), ref=np.max)\n","plt.subplot(4, 2, 1)\n","librosa.display.specshow(D, y_axis='linear')\n","plt.colorbar(format='%+2.0f dB')\n","plt.title('Linear-frequency power spectrogram')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["'''Using random samples to observe difference in waveforms.'''\n","\n","arr = np.array(df[\"slice_file_name\"])\n","fold = np.array(df[\"fold\"])\n","cla = np.array(df[\"class\"])\n","\n","for i in range(192, 197, 2):\n","    path = '../input/urbansound8k/fold' + str(fold[i]) + '/' + arr[i]\n","    data, sampling_rate = librosa.load(path)\n","    plt.figure(figsize=(10, 5))\n","    D = librosa.amplitude_to_db(np.abs(librosa.stft(data)), ref=np.max)\n","    plt.subplot(4, 2, 1)\n","    librosa.display.specshow(D, y_axis='linear')\n","    plt.colorbar(format='%+2.0f dB')\n","    plt.title(cla[i])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Feature Extraction and Database Building"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Method\n","\n","1. I have used Librosa to extract features.\n","2. To do so, I will go through each fold and extract the data for each file. Then I have used the mel_spectogram function of librosa to extract the spectogram data as a numpy array.\n","3. After reshaping and cleaning the data, 75-25 split has been performed.\n","4. Classes (Y) have been converted to Categorically Encoded Data usng Keras.utils\n","\n","Note : Running the parser function may take upto 45 minutes depending on your system since it has to extract spectogram data for 8732 audio files"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["'''EXAMPLE'''\n","\n","dat1, sampling_rate1 = librosa.load('../input/urbansound8k/fold5/100032-3-0-0.wav')\n","arr = librosa.feature.melspectrogram(y=dat1, sr=sampling_rate1)\n","arr.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["feature = []\n","label = []\n","\n","def parser(row):\n","    # Function to load files and extract features\n","    for i in range(8732):\n","        file_name = '../input/urbansound8k/fold' + str(df[\"fold\"][i]) + '/' + df[\"slice_file_name\"][i]\n","        # Here kaiser_fast is a technique used for faster extraction\n","        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n","        # We extract mfcc feature from data\n","        mels = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)        \n","        feature.append(mels)\n","        label.append(df[\"classID\"][i])\n","    return [feature, label]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["temp = parser(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["temp = np.array(temp)\n","data = temp.transpose()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_ = data[:, 0]\n","Y = data[:, 1]\n","print(X_.shape, Y.shape)\n","X = np.empty([8732, 128])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(8732):\n","    X[i] = (X_[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["Y = to_categorical(Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["'''Final Data'''\n","print(X.shape)\n","print(Y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train = X_train.reshape(6549, 16, 8, 1)\n","X_test = X_test.reshape(2183, 16, 8, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["input_dim = (16, 8, 1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Creating Keras Model and Testing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Model 1:\n","\n","1. CNN 2D with 64 units and tanh activation.\n","2. MaxPool2D with 2*2 window.\n","3. CNN 2D with 128 units and tanh activation.\n","4. MaxPool2D with 2*2 window.\n","5. Dropout Layer with 0.2 drop probability.\n","6. DL with 1024 units and tanh activation.\n","4. DL 10 units with softmax activation.\n","5. Adam optimizer with categorical_crossentropy loss function.\n","\n","90 epochs have been used."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = Sequential()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.add(Conv2D(64, (3, 3), padding = \"same\", activation = \"tanh\", input_shape = input_dim))\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Conv2D(128, (3, 3), padding = \"same\", activation = \"tanh\"))\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.1))\n","model.add(Flatten())\n","model.add(Dense(1024, activation = \"tanh\"))\n","model.add(Dense(10, activation = \"softmax\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.fit(X_train, Y_train, epochs = 90, batch_size = 50, validation_data = (X_test, Y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions = model.predict(X_test)\n","score = model.evaluate(X_test, Y_test)\n","print(score)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds = np.argmax(predictions, axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["result = pd.DataFrame(preds)\n","result.to_csv(\"UrbanSound8kResults.csv\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
