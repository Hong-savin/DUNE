{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Sound Classification using PyTorch\n",
    "\n",
    "\n",
    "This project aims to develop an artificial intelligence system capable of classifying live sounds using the PyTorch framework. The goal is to build a simple yet effective sound classification model that can accurately identify different types of sounds in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MODELS.ai_0014_MARK1 import SoundClassifier_MARK1\n",
    "from MODELS.ai_0014_MARK2 import SoundClassifier_MARK2\n",
    "from MODELS.ai_0014_MARK3 import SoundClassifier_MARK3\n",
    "from GPU_torch import GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple device detected\n",
      "Activating Apple Silicon GPU\n",
      "current device :  mps\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = GPU()\n",
    "print(\"current device : \",device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple device detected\n",
      "Activating Apple Silicon GPU\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "##IDK what is wrong with that up cell\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "def GPU():\n",
    "    if torch.cuda.is_available() == True:\n",
    "        device = 'cuda'\n",
    "        templist = [1, 2, 3]\n",
    "        templist = torch.FloatTensor(templist).to(device)\n",
    "        print(\"Cuda torch working : \", end=\"\")\n",
    "        print(templist.is_cuda)\n",
    "        print(\"current device no. : \", end=\"\")\n",
    "        print(torch.cuda.current_device())\n",
    "        print(\"GPU device count : \", end=\"\")\n",
    "        print(torch.cuda.device_count())\n",
    "        print(\"GPU name : \", end=\"\")\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print(\"device : \", device)\n",
    "        # Execute the nvidia-smi command using subprocess\n",
    "        try:\n",
    "            output = subprocess.check_output(['nvidia-smi']).decode('utf-8')\n",
    "            print(\"nvidia-smi output:\")\n",
    "            print(output)\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError) as e:\n",
    "            print(\"Error executing nvidia-smi command:\", str(e))\n",
    "    elif torch.backends.mps.is_available() == True:\n",
    "        print(\"Apple device detected\\nActivating Apple Silicon GPU\")\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        print(\"cant use gpu , activating cpu\")\n",
    "        device = 'cpu'\n",
    "\n",
    "    return device\n",
    "device = GPU()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SEED Everything'''\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "def seed_everything(SEED=42):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True # keep True if all the input have same size.\n",
    "SEED=42\n",
    "seed_everything(SEED=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip(\n",
      "  audio_path=\"/Users/cafalena/sound_datasets/urbansound8k/audio/fold3/65750-3-3-48.wav\",\n",
      "  clip_id=\"65750-3-3-48\",\n",
      "  audio: The clip's audio\n",
      "            * np.ndarray - audio signal\n",
      "            * float - sample rate,\n",
      "  class_id: The clip's class id.\n",
      "            * int - integer representation of the class label (0-9). See Dataset Info in the documentation for mapping,\n",
      "  class_label: The clip's class label.\n",
      "            * str - string class name: air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, siren, street_music,\n",
      "  fold: The clip's fold.\n",
      "            * int - fold number (1-10) to which this clip is allocated. Use these folds for cross validation,\n",
      "  freesound_end_time: The clip's end time in Freesound.\n",
      "            * float - end time in seconds of the clip in the original freesound recording,\n",
      "  freesound_id: The clip's Freesound ID.\n",
      "            * str - ID of the freesound.org recording from which this clip was taken,\n",
      "  freesound_start_time: The clip's start time in Freesound.\n",
      "            * float - start time in seconds of the clip in the original freesound recording,\n",
      "  salience: The clip's salience.\n",
      "            * int - annotator estimate of class sailence in the clip: 1 = foreground, 2 = background,\n",
      "  slice_file_name: The clip's slice filename.\n",
      "            * str - The name of the audio file. The name takes the following format: [fsID]-[classID]-[occurrenceID]-[sliceID].wav,\n",
      "  tags: The clip's tags.\n",
      "            * annotations.Tags - tag (label) of the clip + confidence. In UrbanSound8K every clip has one tag,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import soundata\n",
    "\n",
    "    dataset = soundata.initialize('urbansound8k')\n",
    "    #dataset.download()  # download the dataset\n",
    "    #dataset.validate()  # validate that all the expected files are there\n",
    "\n",
    "    example_clip = dataset.choice_clip()  # choose a random example clip\n",
    "    print(example_clip)  # see the available data\n",
    "except:\n",
    "    print(\"SKIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "\n",
    "class UrbanSoundDataset(torch.utils.data.Dataset):  \n",
    "    def __init__(self, annotations, audio_dir):  \n",
    "        if isinstance(annotations, pd.DataFrame):  \n",
    "            self.annotations = annotations  \n",
    "        else:\n",
    "            self.annotations = pd.read_csv(annotations)\n",
    "        self.audio_dir = audio_dir  # add this line to save audio directory\n",
    "\n",
    "    def __len__(self):  \n",
    "        return len(self.annotations)  \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_path = os.path.join(self.audio_dir,\"fold\"+str(self.annotations.loc[index]['fold']),self.annotations.loc[index, 'slice_file_name']) \n",
    "        class_id = self.annotations.loc[index, 'classID']\n",
    "        audio, _ = librosa.load(audio_path, sr=44100, mono=True)  # setting standard sampling rate\n",
    "\n",
    "        # Handle variable length audio files\n",
    "        fixed_length = 44100 * 4  # 4 seconds // datashape (batch, 44100*4)\n",
    "        if len(audio) < fixed_length:\n",
    "            audio = np.pad(audio, (0, fixed_length - len(audio)))  # pad with zeros (zeros are silence)\n",
    "        elif len(audio) > fixed_length:\n",
    "            audio = audio[:fixed_length]  # trim to fixed length\n",
    "        # Transform audio into Mel spectrogram\n",
    "        return torch.from_numpy(audio), torch.tensor([class_id]) \n",
    "\n",
    "\n",
    "\n",
    "#dataset = UrbanSoundDataset()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 (미적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/ 전처리 나중에 적용\n",
    "\n",
    "def calc_fft(y, rate):\n",
    "    n = len(y)\n",
    "    freq = np.fft.rfftfreq(n, d=1/rate)\n",
    "    Y = abs(np.fft.rfft(y)/n)\n",
    "    return Y, freq\n",
    "\n",
    "def plot_signal_fft(signal, rate):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(20, 10))\n",
    "    axs[0].plot(signal)\n",
    "    axs[0].set_title('Signal')\n",
    "    Y, freq = calc_fft(signal, rate)\n",
    "    axs[1].plot(freq, Y)\n",
    "    axs[1].set_title('FFT')\n",
    "    plt.show()\n",
    "\n",
    "def calc_spectrogram(signal, rate):\n",
    "    n_fft = 2048\n",
    "    hop_length = 512\n",
    "    spectrogram = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length)\n",
    "    spectrogram = np.abs(spectrogram)\n",
    "    log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "    return log_spectrogram\n",
    "\n",
    "def plot_spectrogram(signal, rate):\n",
    "    log_spectrogram = calc_spectrogram(signal, rate)\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    axs.imshow(log_spectrogram, aspect='auto', origin='lower', cmap='jet')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "try: \n",
    "    csvdataset = pd.read_csv('/Users/cafalena/sound_datasets/urbansound8k/UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "except:\n",
    "    csvdataset = pd.read_csv('C:/Users/PC/AppData/@FOLDER/@Project/UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into 80% training and 20% temporary\n",
    "train_data, temp = train_test_split(csvdataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reset index\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Split the temporary set into 50% validation and 50% testing\n",
    "validation_data, test_data = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Reset index\n",
    "validation_data = validation_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Now, `train_data` is your training set (80% of total), \n",
    "# `validation_data` is your validation set (10% of total), and \n",
    "# `test_data` is your testing set (10% of total)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCHSIZE = 4\n",
    "\n",
    "\n",
    "audio_dir = \"/Users/cafalena/sound_datasets/urbansound8k/UrbanSound8K/audio\"\n",
    "\n",
    "train_set = UrbanSoundDataset(train_data, audio_dir)\n",
    "val_set = UrbanSoundDataset(validation_data, audio_dir)\n",
    "test_set = UrbanSoundDataset(test_data, audio_dir)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCHSIZE, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCHSIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCHSIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR result\n",
    "\n",
    "LR : 100  Epoch [1/10], Loss: 123043.2521\n",
    "LR : 100  Epoch [2/10], Loss: 7324.8045\n",
    "LR : 100  Epoch [3/10], Loss: 461.2951\n",
    "LR : 100  Epoch [4/10], Loss: 100.8706\n",
    "LR : 100  Epoch [5/10], Loss: 65.5783\n",
    "LR : 100  Epoch [6/10], Loss: 346.9864\n",
    "LR : 100  Epoch [7/10], Loss: 63.1574\n",
    "LR : 100  Epoch [8/10], Loss: 56.8437\n",
    "LR : 100  Epoch [9/10], Loss: 61.7810\n",
    "LR : 100  Epoch [10/10], Loss: 68.3983\n",
    "LR : 10  Epoch [1/10], Loss: 1080.9213\n",
    "LR : 10  Epoch [2/10], Loss: 35.1927\n",
    "LR : 10  Epoch [3/10], Loss: 4.9769\n",
    "LR : 10  Epoch [4/10], Loss: 2.4054\n",
    "LR : 10  Epoch [5/10], Loss: 3.5182\n",
    "LR : 10  Epoch [6/10], Loss: 3.4323\n",
    "LR : 10  Epoch [7/10], Loss: 3.0489\n",
    "LR : 10  Epoch [8/10], Loss: 5.0180\n",
    "LR : 10  Epoch [9/10], Loss: 2.3545\n",
    "LR : 10  Epoch [10/10], Loss: 2.4156\n",
    "LR : 1  Epoch [1/10], Loss: 13.9965\n",
    "LR : 1  Epoch [2/10], Loss: 2.9778\n",
    "LR : 1  Epoch [3/10], Loss: 2.3417\n",
    "LR : 1  Epoch [4/10], Loss: 2.3440\n",
    "LR : 1  Epoch [5/10], Loss: 2.3113\n",
    "LR : 1  Epoch [6/10], Loss: 2.3460\n",
    "LR : 1  Epoch [7/10], Loss: 2.2775\n",
    "LR : 1  Epoch [8/10], Loss: 2.2869\n",
    "LR : 1  Epoch [9/10], Loss: 2.2935\n",
    "LR : 1  Epoch [10/10], Loss: 2.2680\n",
    "LR : 0.1  Epoch [1/10], Loss: 3.1002\n",
    "LR : 0.1  Epoch [2/10], Loss: 2.2569\n",
    "LR : 0.1  Epoch [3/10], Loss: 2.2446\n",
    "LR : 0.1  Epoch [4/10], Loss: 2.2292\n",
    "LR : 0.1  Epoch [5/10], Loss: 2.2173\n",
    "LR : 0.1  Epoch [6/10], Loss: 2.2035\n",
    "LR : 0.1  Epoch [7/10], Loss: 2.1767\n",
    "LR : 0.1  Epoch [8/10], Loss: 2.1629\n",
    "LR : 0.1  Epoch [9/10], Loss: 2.146\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1746 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cafalena/HOUSE/@Code/Practice/ai_0014_CNN_Urbansound/ai_0014_CNN_Urbansound.ipynb 셀 15\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_CNN_Urbansound.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_CNN_Urbansound.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_CNN_Urbansound.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem() \u001b[39m*\u001b[39m data\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_CNN_Urbansound.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m epoch_loss \u001b[39m=\u001b[39m running_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_set)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_CNN_Urbansound.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLR : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m  Epoch [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(LR,epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, NB_EPOCH, epoch_loss))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#LR_list = [100,10,1,1e-1,1e-2,1e-3,1e-5,1e-7,1e-10]\n",
    "#paremeters\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "LR_list = [1e-2]\n",
    "NB_EPOCH = 3\n",
    "num_classes = 10  # for UrbanSound8K dataset, there are 10 classes\n",
    "input_size = 44100 * 4  # 44100Hz * 4 seconds\n",
    "#audio_dir = \"C:/Users/PC/AppData/@FOLDER/@Project/UrbanSound8K/audio\"\n",
    "audio_dir = \"/Users/cafalena/sound_datasets/urbansound8k/UrbanSound8K/audio\"\n",
    "#Saving the best model\n",
    "min_loss = float('inf')\n",
    "\n",
    "for LR in LR_list:\n",
    "\n",
    "    #// we have to change that it uses many lr \n",
    "    model = SoundClassifier_MARK2(input_size, num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for epoch in range(NB_EPOCH):\n",
    "        running_loss = 0.0\n",
    "        for data, target in tqdm(train_loader): # tqdm\n",
    "        #for data, target in (train_loader): #no tqdm\n",
    "            data = data.unsqueeze(1)\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "        epoch_loss = running_loss / len(train_set)\n",
    "        print('LR : {}  Epoch [{}/{}], Loss: {:.4f}'.format(LR,epoch+1, NB_EPOCH, epoch_loss))\n",
    "        # Save the BEST model if the current epoch loss is less than the minimum loss so far\n",
    "#------------------------------- Valdation\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data = data.unsqueeze(1)\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target.squeeze())\n",
    "                val_running_loss += loss.item() * data.size(0)\n",
    "        val_epoch_loss = val_running_loss / len(val_set)\n",
    "        print('LR : {}  Epoch [{}/{}], Validation Loss: {:.4f}'.format(LR, epoch+1, NB_EPOCH, val_epoch_loss))\n",
    "\n",
    "        # Save the BEST model if the current epoch validation loss is less than the minimum loss so far\n",
    "        if val_epoch_loss < min_loss:\n",
    "            print(\"Saving the best model with validation loss: {:.4f}\".format(val_epoch_loss))\n",
    "            min_loss = val_epoch_loss\n",
    "            torch.save(model.state_dict(), \"MARK2_best.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [01:08<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the validation set: 12.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"MARK2_best.pth\"))\n",
    "model.eval()\n",
    "\n",
    "#I dont know why, but If I use GPU(mps) an error accurs\n",
    "device = 'cpu'\n",
    "model.to('cpu')\n",
    "#model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, target in tqdm(test_loader):\n",
    "        #data = data.unsqueeze(0)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data,1)##IMPORTANT if your model dont use batchnorm, use max(output.data,0) instead\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.squeeze()).sum().item()\n",
    "    print('Accuracy of the model on the validation set: {:.2f}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCHSIZE = 64\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# try:\n",
    "#     testdataset = pd.read_csv('/Users/cafalena/sound_datasets/urbansound8k/UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "# except:\n",
    "#     testdataset = pd.read_csv(\"C:/Users/PC/AppData/@FOLDER/@Project/UrbanSound8K/metadata/UrbanSound8K.csv\")\n",
    "    \n",
    "# num_classes = 10  # for UrbanSound8K dataset, there are 10 classes\n",
    "# input_size = 44100 * 4  # 44100Hz * 4 seconds\n",
    "\n",
    "# test_loader = UrbanSoundDataset(testdataset)\n",
    "# train_loader = torch.utils.data.DataLoader(test_loader, batch_size=BATCHSIZE, shuffle=True)\n",
    "# model = SoundClassifier_MARK2(input_size, num_classes)\n",
    "# model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "# model.eval()\n",
    "\n",
    "# #model.to('cpu')\n",
    "# model.to(device)\n",
    "# with torch.no_grad():\n",
    "#     print(4)\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for data, target in tqdm(train_loader):\n",
    "#         data = data.unsqueeze(0)\n",
    "#         data = data.to(device)\n",
    "#         target = target.to(device)\n",
    "        \n",
    "#         output = model(data)\n",
    "#         _, predicted = torch.max(output.data,1)##IMPORTANT if your model dont use batchnorm, use max(output.data,0) instead\n",
    "#         total += target.size(0)\n",
    "#         correct += (predicted == target.squeeze()).sum().item()\n",
    "#     print('Accuracy of the model on the validation set: {:.2f}%'.format(100 * correct / total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diary\n",
    "\n",
    "**5/14 1300**: I have resolved the folder location issue, but now I am facing a new problem. The folder location and the sound file do not match. It's strange because the folder and the CSV files are fine. However, the `audio_path` is pointing in the wrong direction.\n",
    "\n",
    "**5/14 1310**: I noticed that the folder and the file name were slightly off, which indicates that it's not entirely random. So, I decided to avoid using split and shuffle. Surprisingly, it worked. It seems like the split function was causing the problem, but I will keep monitoring the situation. Although the location error still persists, I tried specifying the complete location path as \"/Users/cafalena/sound_datasets/urbansound8k/UrbanSound8K/\". This resolved the issue, and I observed that it recognized multiple sound files. However, now I am facing a tensor problem. I need to address this next.\n",
    "\n",
    "**5/14 1752**: I was planning to use a CNN (Convolutional Neural Network), but I realized that sound waves are 1-dimensional. I'm struggling to figure out how to utilize a CNN with sound. Therefore, for now, I will stick with an NN (Neural Network). Once I successfully implement the NN, I can revisit using a CNN. Additionally, I need to work on the accuracy and test code sections.  \n",
    "\n",
    "**5/15 1530** I solved the problem with NN models matmul (matix muliply) problem. The problem was about the batch norm and dim (if the dim is 1 more, you need unsqueeze please check .shape())  \n",
    "\n",
    "**5/17 1915** I attempted to download the YouTube AudioSet, but encountered several issues and was unsuccessful. As an alternative, I downloaded another dataset from AIHUB. However, I am uncertain if I will utilize it due to the extensive processing required. To make progress at this moment, I need to focus on implementing the Convolutional Neural Network (CNN) and signal processing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
