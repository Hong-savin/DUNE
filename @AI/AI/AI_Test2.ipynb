{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mic check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'MacBook Air 마이크', 'index': 3, 'hostapi': 0, 'max_input_channels': 1, 'max_output_channels': 0, 'default_low_input_latency': 0.0336875, 'default_low_output_latency': 0.01, 'default_high_input_latency': 0.043020833333333335, 'default_high_output_latency': 0.1, 'default_samplerate': 48000.0}\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "\n",
    "def get_default_input_device_info():\n",
    "    default_input_device = sd.default.device[0]  # get the ID of the default input device\n",
    "    device_info = sd.query_devices(default_input_device)\n",
    "    print(device_info)\n",
    "\n",
    "# Get the info of the default input device\n",
    "get_default_input_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device #1 name: 갤럭시 S2 마이크\n",
      "Device #2 name: USB PnP Sound Device\n",
      "Device #3 name: MacBook Air 마이크\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "def list_input_devices():\n",
    "    devices = sd.query_devices()\n",
    "    for i, device in enumerate(devices):\n",
    "        if device['max_input_channels'] > 0:  # this is an input device\n",
    "            print(f\"Device #{i} name: {device['name']}\")\n",
    "\n",
    "# List available input devices (including microphones)\n",
    "list_input_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Choose the device to use for recording\n",
    "device_id = 2  # replace with the ID of the device you want to use\n",
    "duration = 3  # seconds\n",
    "\n",
    "# Create a buffer to store the audio data\n",
    "buffer = np.zeros((duration * 44100,))\n",
    "buffer_index = 0\n",
    "\n",
    "# Define a callback function to process the audio input\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    global buffer_index\n",
    "    volume_norm = np.linalg.norm(indata) * 10\n",
    "    print(f'\\r{\"|\" * int(volume_norm)}', end='')  # print a simple \"volume bar\"\n",
    "\n",
    "    # Store the incoming data in the buffer\n",
    "    buffer[buffer_index:buffer_index+frames] = indata[:, 0]\n",
    "    buffer_index += frames\n",
    "\n",
    "# Create a stream object\n",
    "stream = sd.InputStream(callback=audio_callback, device=device_id, channels=1, samplerate=44100)\n",
    "\n",
    "# Start the stream\n",
    "with stream:\n",
    "    # Record for 3 seconds\n",
    "    sd.sleep(duration * 1000)\n",
    "\n",
    "# Play back the recorded sound\n",
    "sd.play(buffer, samplerate=44100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Model2_005.pth\n",
      "2. Model2_006.pth\n",
      "3. Model2_007.pth\n",
      "4. Model2_008.pth\n",
      "5. Model2_009.pth\n",
      "6. Model2_010.pth\n",
      "7. Batch4-1k_001.pth\n",
      "8. Batch4-1k_002.pth\n",
      "9. Batch4-1k_003.pth\n",
      "10. Batch4-1k_004.pth\n",
      "11. Model2_001.pth\n",
      "12. Model2_002.pth\n",
      "13. Model2_003.pth\n",
      "14. Model2_004.pth\n",
      "Model loaded from models/Batch4-1k_004.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "def load_model_interactive(model_dir=\"models\"):\n",
    "    # List all models in the directory\n",
    "    model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth') and not f.startswith('._')]\n",
    "    \n",
    "    # Display the models to the user\n",
    "    for idx, model_name in enumerate(model_files, 1):\n",
    "        print(f\"{idx}. {model_name}\")\n",
    "    \n",
    "    # Get user input\n",
    "    selected_idx = int(input(\"Enter the number corresponding to the model you wish to load: \")) - 1\n",
    "    model_path = os.path.join(model_dir, model_files[selected_idx])\n",
    "    model = None   \n",
    "    try:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            \n",
    "            # Load the resnet18 model structure\n",
    "            model = resnet18(pretrained=False)\n",
    "            num_ftrs = model.fc.in_features\n",
    "            model.fc = nn.Linear(512, 14)  # Assuming selected_labels is globally accessible\n",
    "            \n",
    "            # Load the state dict\n",
    "            state_dict = torch.load(model_path, map_location=device)\n",
    "            new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "            model.load_state_dict(new_state_dict)\n",
    "            \n",
    "            print(f\"Model loaded from {model_path}\")\n",
    "            model = model.to(device)\n",
    "            model = model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the model. Error: {e}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# For demonstration purposes, the function will display model names but won't actually load them here.\n",
    "# Please run this in your local environment for actual model loading.\n",
    "model = load_model_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxNormalize(nn.Module):\n",
    "    def __init__(self, min_val=None, max_val=None):\n",
    "        super(MinMaxNormalize, self).__init__()\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        if self.min_val is None or self.max_val is None:\n",
    "            min_val = torch.min(tensor)\n",
    "            max_val = torch.max(tensor)\n",
    "        else:\n",
    "            min_val = self.min_val\n",
    "            max_val = self.max_val\n",
    "        \n",
    "        normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "        return normalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonoToColor(nn.Module):\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(MonoToColor, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor.repeat(self.num_channels, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "# Apply the same transformation as used during training\n",
    "transformation = transforms.Compose([\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=40),# higher the better but more complex. For talking we use 128, for sound effect, about 40.\n",
    "    torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80),\n",
    "    MinMaxNormalize(),\n",
    "    MonoToColor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / Predicted class: Nothing e / Max Probability: 88.65%\n",
      "1 / Predicted class: Nothing h / Max Probability: 51.20%\n",
      "2 / Predicted class: Nothing d / Max Probability: 55.97%\n",
      "3 / Predicted class: Nothing e / Max Probability: 72.38%\n",
      "4 / Predicted class: Nothing d / Max Probability: 80.13%\n",
      "5 / Predicted class: Nothing e / Max Probability: 80.37%\n",
      "6 / Predicted class: Nothing h / Max Probability: 91.28%\n",
      "7 / Predicted class: Nothing h / Max Probability: 90.40%\n",
      "8 / Predicted class: Dog Bark / Max Probability: 75.20%\n",
      "9 / Predicted class: Nothing e / Max Probability: 85.84%\n",
      "10 / Predicted class: Nothing h / Max Probability: 27.63%\n",
      "11 / Predicted class: Nothing h / Max Probability: 73.96%\n",
      "12 / Predicted class: Bell Ring / Max Probability: 95.46%\n",
      "13 / Predicted class: Door Nock / Max Probability: 82.67%\n",
      "14 / Predicted class: Nothing d / Max Probability: 29.41%\n",
      "15 / Predicted class: Dog Bark / Max Probability: 40.52%\n",
      "16 / Predicted class: Door Nock / Max Probability: 70.89%\n",
      "17 / Predicted class: Dog Bark / Max Probability: 99.99%\n",
      "18 / Predicted class: Dog Bark / Max Probability: 99.96%\n",
      "19 / Predicted class: Dog Bark / Max Probability: 99.99%\n",
      "20 / Predicted class: Dog Bark / Max Probability: 99.47%\n",
      "21 / Predicted class: Dog Bark / Max Probability: 99.97%\n",
      "22 / Predicted class: Dog Bark / Max Probability: 99.99%\n",
      "23 / Predicted class: Dog Bark / Max Probability: 98.41%\n",
      "24 / Predicted class: Dog Bark / Max Probability: 96.92%\n",
      "25 / Predicted class: Nothing h / Max Probability: 90.04%\n",
      "26 / Predicted class: Dog Bark / Max Probability: 99.22%\n",
      "27 / Predicted class: Dog Bark / Max Probability: 99.40%\n",
      "28 / Predicted class: Car Horn / Max Probability: 99.71%\n",
      "29 / Predicted class: Car Horn / Max Probability: 99.87%\n",
      "30 / Predicted class: Car Horn / Max Probability: 99.36%\n",
      "31 / Predicted class: Car Horn / Max Probability: 99.21%\n",
      "32 / Predicted class: Car Horn / Max Probability: 99.29%\n",
      "33 / Predicted class: Dog Bark / Max Probability: 100.00%\n",
      "34 / Predicted class: Dog Bark / Max Probability: 99.98%\n",
      "35 / Predicted class: Car Horn / Max Probability: 99.00%\n",
      "36 / Predicted class: Dog Bark / Max Probability: 94.33%\n",
      "37 / Predicted class: Dog Bark / Max Probability: 89.77%\n",
      "38 / Predicted class: Dog Bark / Max Probability: 99.98%\n",
      "39 / Predicted class: Car Horn / Max Probability: 99.96%\n",
      "40 / Predicted class: Dog Bark / Max Probability: 99.97%\n",
      "41 / Predicted class: Nothing h / Max Probability: 72.84%\n",
      "42 / Predicted class: Nothing f / Max Probability: 59.76%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished continuous sound prediction.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 52\u001b[0m continuous_sound_prediction(model, device, transformation, SAMPLE_RATE, device_id)\n",
      "Cell \u001b[0;32mIn[46], line 15\u001b[0m, in \u001b[0;36mcontinuous_sound_prediction\u001b[0;34m(model, device, transformation, sample_rate, device_id)\u001b[0m\n\u001b[1;32m     13\u001b[0m duration \u001b[39m=\u001b[39m \u001b[39m2.0\u001b[39m  \u001b[39m# seconds\u001b[39;00m\n\u001b[1;32m     14\u001b[0m recording \u001b[39m=\u001b[39m sd\u001b[39m.\u001b[39mrec(\u001b[39mint\u001b[39m(duration \u001b[39m*\u001b[39m sample_rate), samplerate\u001b[39m=\u001b[39msample_rate, channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, device\u001b[39m=\u001b[39mdevice_id)\n\u001b[0;32m---> 15\u001b[0m sd\u001b[39m.\u001b[39mwait()\n\u001b[1;32m     17\u001b[0m \u001b[39m# Preprocessing\u001b[39;00m\n\u001b[1;32m     18\u001b[0m recording \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(recording)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/sounddevice.py:395\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ignore_errors)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for `play()`/`rec()`/`playrec()` to be finished.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[1;32m    381\u001b[0m \u001b[39mPlayback/recording can be stopped with a `KeyboardInterrupt`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m _last_callback:\n\u001b[0;32m--> 395\u001b[0m     \u001b[39mreturn\u001b[39;00m _last_callback\u001b[39m.\u001b[39mwait(ignore_errors)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/sounddevice.py:2601\u001b[0m, in \u001b[0;36m_CallbackContext.wait\u001b[0;34m(self, ignore_errors)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for finished_callback.\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m \n\u001b[1;32m   2597\u001b[0m \u001b[39mCan be interrupted with a KeyboardInterrupt.\u001b[39;00m\n\u001b[1;32m   2598\u001b[0m \n\u001b[1;32m   2599\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2600\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mwait()\n\u001b[1;32m   2602\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2603\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mclose(ignore_errors)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cond\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    623\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def continuous_sound_prediction(model, device, transformation, sample_rate, device_id):\n",
    "    labels = [\n",
    "        \"nothing a\", \"Car Horn\", \"Bell Ring\", \"Dog Bark\", \"nothing b\", \n",
    "        \"nothing c\", \"Glass Shatter\", \"Nothing d\", \"Nothing e\", \"Door Nock\", \n",
    "        \"Nothing f\", \"Nothing g\", \"Siren\", \"Nothing h\"\n",
    "    ]\n",
    "\n",
    "    for count in range(101):\n",
    "        try:\n",
    "            # Recording\n",
    "            duration = 2.0  # seconds\n",
    "            recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, device=device_id)\n",
    "            sd.wait()\n",
    "            \n",
    "            # Preprocessing\n",
    "            recording = torch.from_numpy(recording).float().transpose(0, 1)\n",
    "            if recording.shape[0] > 1:\n",
    "                recording = torch.mean(recording, dim=0, keepdim=True)\n",
    "            recording = nn.functional.pad(recording, (0, sample_rate - recording.shape[1]))\n",
    "            \n",
    "            # Transformation\n",
    "            recording = transformation(recording)\n",
    "            \n",
    "            # Prediction\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                recording = recording.to(device)\n",
    "                outputs = model(recording.unsqueeze(0))\n",
    "                #probabilities = F.softmax(outputs, dim=1)\n",
    "                probabilities = torch.sigmoid(outputs)\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "            # Print results\n",
    "            probs = [f\"{label} {prob:.2%}\" for label, prob in zip(labels, probabilities[0])]\n",
    "            #print(f\"{count} / {' / '.join(probs)}\")\n",
    "\n",
    "\n",
    "\n",
    "            max_prob, predicted_label_idx = probabilities[0].max(0)\n",
    "            max_prob_label = labels[predicted_label_idx]\n",
    "            print(f\"{count} / Predicted class: {max_prob_label} / Max Probability: {max_prob:.2%}\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "            break\n",
    "\n",
    "    print(\"Finished continuous sound prediction.\")\n",
    "device = 'cpu'\n",
    "continuous_sound_prediction(model, device, transformation, SAMPLE_RATE, device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||PaMacCore (AUHAL)|| Error on line 2523: err='-50', msg=Unknown Error\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 134\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished continuous sound prediction.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 134\u001b[0m continuous_sound_prediction(model, device, transformation, SAMPLE_RATE, device_id)\n",
      "Cell \u001b[0;32mIn[48], line 105\u001b[0m, in \u001b[0;36mcontinuous_sound_prediction\u001b[0;34m(model, device, transformation, sample_rate, device_id)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mfor\u001b[39;00m count \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m101\u001b[39m):\n\u001b[1;32m    103\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m         \u001b[39m# Recording\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m         audio_data \u001b[39m=\u001b[39m record_audio(device_id, sample_rate\u001b[39m=\u001b[39msample_rate)\n\u001b[1;32m    107\u001b[0m         \u001b[39m# Preprocessing\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         audio_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(audio_data)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[48], line 23\u001b[0m, in \u001b[0;36mrecord_audio\u001b[0;34m(input_device_index, sample_rate, channels, duration)\u001b[0m\n\u001b[1;32m     20\u001b[0m frames \u001b[39m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mint\u001b[39m(sample_rate \u001b[39m/\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m duration)):\n\u001b[0;32m---> 23\u001b[0m     data \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39mread(\u001b[39m1024\u001b[39m)\n\u001b[1;32m     24\u001b[0m     frames\u001b[39m.\u001b[39mappend(data)\n\u001b[1;32m     26\u001b[0m stream\u001b[39m.\u001b[39mstop_stream()\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/pyaudio/__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[0;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_input:\n\u001b[1;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNot input stream\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m--> 570\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mread_stream(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream, num_frames,\n\u001b[1;32m    571\u001b[0m                       exception_on_overflow)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "# Modified record_audio function\n",
    "def record_audio(input_device_index, sample_rate=44100, channels=1, duration=2):\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=channels,\n",
    "                    rate=sample_rate,\n",
    "                    input=True,\n",
    "                    input_device_index=input_device_index,\n",
    "                    frames_per_buffer=1024)\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(0, int(sample_rate / 1024 * duration)):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    # Convert frames to numpy array\n",
    "    byte_string = b''.join(frames)\n",
    "    audio_array = np.frombuffer(byte_string, dtype=np.int16)\n",
    "    return audio_array\n",
    "\n",
    "\n",
    "# Additional transformation methods\n",
    "def _right_pad_if_necessary(signal, target_sample_rate):\n",
    "    length_signal = signal.shape[1]\n",
    "    if length_signal < target_sample_rate:\n",
    "        num_missing_samples = target_sample_rate - length_signal\n",
    "        last_dim_padding = (0, num_missing_samples)\n",
    "        signal = nn.functional.pad(signal, last_dim_padding)\n",
    "    return signal\n",
    "\n",
    "def _resample_if_necessary(signal, sr, target_sample_rate):\n",
    "    if sr != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, target_sample_rate)\n",
    "        signal = resampler(signal)\n",
    "    return signal\n",
    "\n",
    "def _mix_down_if_necessary(signal):\n",
    "    if signal.shape[0] > 1:\n",
    "        signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "    return signal\n",
    "\n",
    "\n",
    "# MonoToColor and MinMaxNormalize classes\n",
    "class MonoToColor(nn.Module):\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(MonoToColor, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor.repeat(self.num_channels, 1, 1)\n",
    "\n",
    "class MinMaxNormalize(nn.Module):\n",
    "    def __init__(self, min_val=None, max_val=None):\n",
    "        super(MinMaxNormalize, self).__init__()\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        if self.min_val is None or self.max_val is None:\n",
    "            min_val = torch.min(tensor)\n",
    "            max_val = torch.max(tensor)\n",
    "        else:\n",
    "            min_val = self.min_val\n",
    "            max_val = self.max_val\n",
    "        \n",
    "        normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "        return normalized_tensor\n",
    "\n",
    "\n",
    "# Apply the same transformation as used during training\n",
    "SAMPLE_RATE = 22050\n",
    "transformation = transforms.Compose([\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=40),\n",
    "    torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80),\n",
    "    MinMaxNormalize(),\n",
    "    MonoToColor()\n",
    "])\n",
    "\n",
    "\n",
    "# Modify continuous_sound_prediction function to use record_audio\n",
    "def continuous_sound_prediction(model, device, transformation, sample_rate, device_id):\n",
    "    labels = [\n",
    "        \"children\", \"nothing2\", \"drilling\", \"engine\", \"siren\", \n",
    "        \"gunshot\", \"aircon\", \"jackhammer\", \"carhorn\", \"glass\", \n",
    "        \"nock\", \"street_music\", \"dog_bark\", \"nothing1\"\n",
    "    ]\n",
    "\n",
    "    for count in range(101):\n",
    "        try:\n",
    "            # Recording\n",
    "            audio_data = record_audio(device_id, sample_rate=sample_rate)\n",
    "            \n",
    "            # Preprocessing\n",
    "            audio_tensor = torch.from_numpy(audio_data).float().unsqueeze(0)\n",
    "            audio_tensor = _right_pad_if_necessary(audio_tensor, SAMPLE_RATE)\n",
    "            audio_tensor = _mix_down_if_necessary(audio_tensor)\n",
    "            \n",
    "            # Transformation\n",
    "            transformed_audio = transformation(audio_tensor)\n",
    "            \n",
    "            # Prediction\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                transformed_audio = transformed_audio.to(device)\n",
    "                outputs = model(transformed_audio.unsqueeze(0))\n",
    "                probabilities = F.softmax(outputs, dim=1)\n",
    "                _, predicted = outputs.max(1)\n",
    "\n",
    "            # Print results\n",
    "            probs = [f\"{label} {prob:.2%}\" for label, prob in zip(labels, probabilities[0])]\n",
    "            print(f\"{count} / {' / '.join(probs)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "            break\n",
    "\n",
    "    print(\"Finished continuous sound prediction.\")\n",
    "\n",
    "device = 'cpu'\n",
    "continuous_sound_prediction(model, device, transformation, SAMPLE_RATE, device_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
