{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install torch torchaudio sounddevice transformers\n",
    "pip install pyaudio\n",
    "brew install portaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "#from GPU_torch import GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = GPU()\n",
    "#print(\"current device : \",device)\n",
    "#print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple device detected\n",
      "Activating Apple Silicon GPU\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "def GPU():\n",
    "    if torch.cuda.is_available() == True:\n",
    "        device = 'cuda'\n",
    "        templist = [1, 2, 3]\n",
    "        templist = torch.FloatTensor(templist).to(device)\n",
    "        print(\"Cuda torch working : \", end=\"\")\n",
    "        print(templist.is_cuda)\n",
    "        print(\"current device no. : \", end=\"\")\n",
    "        print(torch.cuda.current_device())\n",
    "        print(\"GPU device count : \", end=\"\")\n",
    "        print(torch.cuda.device_count())\n",
    "        print(\"GPU name : \", end=\"\")\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print(\"device : \", device)\n",
    "        # Execute the nvidia-smi command using subprocess\n",
    "        try:\n",
    "            output = subprocess.check_output(['nvidia-smi']).decode('utf-8')\n",
    "            print(\"nvidia-smi output:\")\n",
    "            print(output)\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError) as e:\n",
    "            print(\"Error executing nvidia-smi command:\", str(e))\n",
    "    elif torch.backends.mps.is_available() == True:\n",
    "        print(\"Apple device detected\\nActivating Apple Silicon GPU\")\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        print(\"cant use gpu , activating cpu\")\n",
    "        device = 'cpu'\n",
    "\n",
    "    return device\n",
    "device = GPU()\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pretrained model loading\n",
    "tokenizer = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyaudio params\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "\n",
    "# speech recognition function\n",
    "def recognize_speech_from_mic():\n",
    "    p = pyaudio.PyAudio()\n",
    "    frames = []\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"* recording\")\n",
    "    for i in range(0, int(RATE / CHUNK * 5)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(np.frombuffer(data, dtype=np.int16))\n",
    "\n",
    "    print(\"* done recording\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "\n",
    "    frame = np.concatenate(frames).astype(np.float32)\n",
    "    input_values = tokenizer(frame, return_tensors=\"pt\", padding=\"longest\", sampling_rate=RATE).input_values\n",
    "    input_values = input_values.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = tokenizer.decode(predicted_ids[0])\n",
    "    print(\"Transcription: \", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording\n",
      "* done recording\n",
      "Transcription:  HALLO THIS IS RECORDING FRO\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    recognize_speech_from_mic()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART (translation not finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sounddevice as sd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torchaudio\n",
    "# from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, BertModel, BertTokenizer\n",
    "\n",
    "# # Pretrained model loading\n",
    "# tokenizer = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "# # Korean BERT Model\n",
    "# kor_bert_tokenizer = BertTokenizer.from_pretrained(\"beomi/kcbert-base\")\n",
    "# kor_bert_model = BertModel.from_pretrained(\"beomi/kcbert-base\")\n",
    "\n",
    "# # Check if CUDA is available and set the device accordingly\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# kor_bert_model.to(device)\n",
    "\n",
    "# # pyaudio params\n",
    "# CHUNK = 1024\n",
    "# FORMAT = pyaudio.paInt16\n",
    "# CHANNELS = 1\n",
    "# RATE = 16000\n",
    "\n",
    "# # speech recognition function\n",
    "# def recognize_speech_from_mic():\n",
    "#     p = pyaudio.PyAudio()\n",
    "#     frames = []\n",
    "#     stream = p.open(format=FORMAT,\n",
    "#                     channels=CHANNELS,\n",
    "#                     rate=RATE,\n",
    "#                     input=True,\n",
    "#                     frames_per_buffer=CHUNK)\n",
    "\n",
    "#     print(\"* recording\")\n",
    "#     for i in range(0, int(RATE / CHUNK * 5)):\n",
    "#         data = stream.read(CHUNK)\n",
    "#         frames.append(np.frombuffer(data, dtype=np.int16))\n",
    "\n",
    "#     print(\"* done recording\")\n",
    "\n",
    "#     stream.stop_stream()\n",
    "#     stream.close()\n",
    "#     p.terminate()\n",
    "\n",
    "#     frame = np.concatenate(frames).astype(np.float32)\n",
    "#     input_values = tokenizer(frame, return_tensors=\"pt\", padding=\"longest\", sampling_rate=RATE).input_values\n",
    "#     input_values = input_values.to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(input_values).logits\n",
    "\n",
    "#     predicted_ids = torch.argmax(logits, dim=-1)\n",
    "#     transcription = tokenizer.decode(predicted_ids[0])\n",
    "\n",
    "#     # Tokenize the transcription and pass through Korean BERT model\n",
    "#     inputs = kor_bert_tokenizer(transcription, return_tensors=\"pt\")\n",
    "#     outputs = kor_bert_model(**inputs)\n",
    "\n",
    "#     print(\"Transcription: \", transcription)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     recognize_speech_from_mic()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
