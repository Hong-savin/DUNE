{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Filter the dataframe to only include rows with the selected labels\n",
    "df = df[df['labels'].apply(lambda x: any([label in x for label in selected_labels]))]\n",
    "\n",
    "# One-hot encode the labels\n",
    "df = df.join(pd.DataFrame(mlb.fit_transform(df.pop('labels')),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=df.index))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN\n"
     ]
    }
   ],
   "source": [
    "#if use CNN type 1, if use resnet 18 type 2 , resnet 34 is type 3\n",
    "\n",
    "select = 1\n",
    "if select == 1:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class AudioClassifier(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(AudioClassifier, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 64, 3, padding=1)\n",
    "            self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "            self.fc1 = nn.Linear(128*32*21, 1024)\n",
    "            self.fc2 = nn.Linear(1024, len(mlb.classes_))\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.unsqueeze(1)  # Add an extra dimension for the single channel\n",
    "            x = torch.relu(self.conv1(x))\n",
    "            x = torch.max_pool2d(x, 2)\n",
    "            x = torch.relu(self.conv2(x))\n",
    "            x = torch.max_pool2d(x, 2)\n",
    "            x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "    print('CNN')\n",
    "if select == 2:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torchvision import models\n",
    "\n",
    "    class AudioClassifier(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(AudioClassifier, self).__init__()\n",
    "            self.resnet = models.resnet18(pretrained=True)\n",
    "            \n",
    "            # Modify first layer to take 1-channel input\n",
    "            self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "            \n",
    "            # Modify last layer to have the correct number of output classes\n",
    "            num_features = self.resnet.fc.in_features\n",
    "            self.resnet.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.unsqueeze(1)  # Add an extra dimension for the single channel\n",
    "            x = self.resnet(x)\n",
    "            return x\n",
    "\n",
    "    model = AudioClassifier(num_classes=len(mlb.classes_))\n",
    "if select == 3:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torchvision import models\n",
    "\n",
    "    class AudioClassifier(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(AudioClassifier, self).__init__()\n",
    "            self.resnet = models.resnet34(pretrained=True)\n",
    "            \n",
    "            # Modify first layer to take 1-channel input\n",
    "            self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "            \n",
    "            # Modify last layer to have the correct number of output classes\n",
    "            num_features = self.resnet.fc.in_features\n",
    "            self.resnet.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.unsqueeze(1)  # Add an extra dimension for the single channel\n",
    "            x = self.resnet(x)\n",
    "            return x\n",
    "\n",
    "    model = AudioClassifier(num_classes=len(mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiLabelBinarizer' object has no attribute 'classes_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m mel_spec\n\u001b[1;32m     14\u001b[0m \u001b[39m# Load the trained model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m model \u001b[39m=\u001b[39m AudioClassifier()\n\u001b[1;32m     17\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mAI2.pth\u001b[39m\u001b[39m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mAudioClassifier.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(\u001b[39m64\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m3\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39m128\u001b[39m\u001b[39m*\u001b[39m\u001b[39m32\u001b[39m\u001b[39m*\u001b[39m\u001b[39m21\u001b[39m, \u001b[39m1024\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39m1024\u001b[39m, \u001b[39mlen\u001b[39m(mlb\u001b[39m.\u001b[39mclasses_))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiLabelBinarizer' object has no attribute 'classes_'"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_mel_spectrogram(y, sr=22050, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    # Compute the spectrogram\n",
    "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "    D_abs = np.abs(D)\n",
    "    # Convert to Mel scale\n",
    "    mel_spec = librosa.feature.melspectrogram(S=librosa.amplitude_to_db(D_abs), sr=sr, n_mels=n_mels)\n",
    "    return mel_spec\n",
    "\n",
    "# Load the trained model\n",
    "model = AudioClassifier()\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load('AI2.pth'))\n",
    "except:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load('AI2.pth', map_location=torch.device('cpu')))\n",
    "    except:\n",
    "        state_dict = torch.load('AI2.pth', map_location=torch.device('cpu'))\n",
    "        new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.eval()\n",
    "\n",
    "# Record audio\n",
    "duration = 2  # seconds\n",
    "fs = 44100  # Sample rate\n",
    "recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "\n",
    "# Preprocess audio\n",
    "recording = np.squeeze(recording)  # Remove the singleton dimension\n",
    "recording = librosa.resample(recording, orig_sr=fs, target_sr=22050)  # Resample the recording\n",
    "mel_spec = get_mel_spectrogram(recording)  # Compute the Mel spectrogram\n",
    "\n",
    "# Make prediction\n",
    "mel_spec = torch.from_numpy(mel_spec).unsqueeze(0).float()  # Add two singleton dimensions at the beginning\n",
    "outputs = model(mel_spec)\n",
    "probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "top_prob, top_label = torch.max(probs, dim=1)\n",
    "\n",
    "#print(f\"Predicted label: {le.inverse_transform([top_label.item()])[0]}\")\n",
    "#print(f\"Probability: {top_prob.item() * 100:.2f}%\")\n",
    "# Define the threshold\n",
    "threshold = 0.10\n",
    "\n",
    "# Apply threshold to probabilities\n",
    "high_prob_mask = probs > threshold\n",
    "\n",
    "# Get labels for high-probability predictions\n",
    "high_prob_labels = le.inverse_transform(torch.where(high_prob_mask)[1])\n",
    "\n",
    "print(f\"Predicted labels: {high_prob_labels}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
