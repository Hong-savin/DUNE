{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cafalena/opt/anaconda3/envs/torchenv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/cafalena/opt/anaconda3/envs/torchenv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded.+CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.autograd import Variable\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "try:\n",
    "    # MULTI GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = resnet18(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(512, 10)\n",
    "    model = nn.DataParallel(model)  # Add this line\n",
    "    model.load_state_dict(torch.load('ResNet18_Best.pth', map_location=device))\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    state_dict = torch.load('ResNet18_Best.pth', map_location=device)\n",
    "    new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "    print(\"Model successfully loaded. + GPU\")\n",
    "except:\n",
    "    #One GPU or CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = resnet18(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(512, 10)\n",
    "    try:\n",
    "        state_dict = torch.load('ResNet18_Best.pth', map_location=device)\n",
    "        new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        model.load_state_dict(new_state_dict)\n",
    "        model = model.to(device)\n",
    "        model = model.eval()\n",
    "        print(\"Model successfully loaded.+CPU\")\n",
    "    except:\n",
    "        print(\"Failed to load the model. Please check the model file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cafalena/opt/anaconda3/envs/torchenv/lib/python3.8/site-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "\n",
    "class MonoToColor(nn.Module):\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(MonoToColor, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor.repeat(self.num_channels, 1, 1)\n",
    "\n",
    "# Apply the same transformation as used during training\n",
    "transformation = transforms.Compose([\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=128),\n",
    "    torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80),\n",
    "    MonoToColor()\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2seconds / 80% upper guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: children_playing, with confidence: 39.81%\n",
      "The predicted class is: street_music, with confidence: 60.54%\n",
      "The predicted class is: street_music, with confidence: 76.84%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cafalena/HOUSE/@Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb 셀 5\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W4sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe predicted class is: \u001b[39m\u001b[39m{\u001b[39;00mpredicted_label\u001b[39m}\u001b[39;00m\u001b[39m, with confidence: \u001b[39m\u001b[39m{\u001b[39;00mpredicted_confidence\u001b[39m:\u001b[39;00m\u001b[39m.2%\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         \u001b[39m# Sleep for 2 seconds before the next prediction\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         \u001b[39m#time.sleep(2.0)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W4sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Call the continuous sound prediction function\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W4sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m continuous_sound_prediction(model, device, transformation, SAMPLE_RATE, SAMPLE_RATE)\n",
      "\u001b[1;32m/Users/cafalena/HOUSE/@Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb 셀 5\u001b[0m in \u001b[0;36mcontinuous_sound_prediction\u001b[0;34m(model, device, transformation, sample_rate, target_sample_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m duration \u001b[39m=\u001b[39m \u001b[39m2.0\u001b[39m  \u001b[39m# seconds\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m recording \u001b[39m=\u001b[39m sd\u001b[39m.\u001b[39mrec(\u001b[39mint\u001b[39m(duration \u001b[39m*\u001b[39m sample_rate), samplerate\u001b[39m=\u001b[39msample_rate, channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m sd\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Convert to PyTorch tensor and switch channels and frames\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m recording \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(recording)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torchenv/lib/python3.8/site-packages/sounddevice.py:395\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ignore_errors)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for `play()`/`rec()`/`playrec()` to be finished.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[1;32m    381\u001b[0m \u001b[39mPlayback/recording can be stopped with a `KeyboardInterrupt`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m _last_callback:\n\u001b[0;32m--> 395\u001b[0m     \u001b[39mreturn\u001b[39;00m _last_callback\u001b[39m.\u001b[39;49mwait(ignore_errors)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torchenv/lib/python3.8/site-packages/sounddevice.py:2601\u001b[0m, in \u001b[0;36m_CallbackContext.wait\u001b[0;34m(self, ignore_errors)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for finished_callback.\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m \n\u001b[1;32m   2597\u001b[0m \u001b[39mCan be interrupted with a KeyboardInterrupt.\u001b[39;00m\n\u001b[1;32m   2598\u001b[0m \n\u001b[1;32m   2599\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2600\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2601\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevent\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m   2602\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2603\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mclose(ignore_errors)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torchenv/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torchenv/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def continuous_sound_prediction(model, device, transformation, sample_rate, target_sample_rate):\n",
    "    # Define class labels\n",
    "    class_labels = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', \n",
    "                    'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music']\n",
    "    \n",
    "    while True:\n",
    "        # Record a 2 seconds mono audio at the specified sample rate\n",
    "        duration = 2.0  # seconds\n",
    "        recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1) \n",
    "        sd.wait()\n",
    "\n",
    "        # Convert to PyTorch tensor and switch channels and frames\n",
    "        recording = torch.from_numpy(recording).float()\n",
    "        recording = torch.transpose(recording, 0, 1)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if sample_rate != target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, target_sample_rate)\n",
    "            recording = resampler(recording)\n",
    "\n",
    "        # Mix down if necessary\n",
    "        if recording.shape[0] > 1:\n",
    "            recording = torch.mean(recording, dim=0, keepdim=True)\n",
    "\n",
    "        # Cut or pad if necessary\n",
    "        if recording.shape[1] > target_sample_rate:\n",
    "            recording = recording[:, :target_sample_rate]\n",
    "        elif recording.shape[1] < target_sample_rate:\n",
    "            num_missing_samples = target_sample_rate - recording.shape[1]\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            recording = nn.functional.pad(recording, last_dim_padding)\n",
    "\n",
    "        # Apply transformation\n",
    "        recording = transformation(recording)\n",
    "\n",
    "        # Make the prediction\n",
    "        model.eval()  # set model to evaluation mode\n",
    "        with torch.no_grad():  # deactivate autograd engine to reduce memory usage and speed up computations\n",
    "            recording = recording.to(device)\n",
    "            outputs = model(recording[None, ...])\n",
    "            probabilities = F.softmax(outputs, dim=1)  # apply softmax to output\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Get predicted label and its corresponding probability\n",
    "        predicted_label = class_labels[predicted.item()]\n",
    "        predicted_confidence = probabilities[0, predicted.item()].item()  # get the probability of the predicted class\n",
    "\n",
    "        # Only print the output if the confidence is greater than 80%\n",
    "        if predicted_confidence >= 0.0:\n",
    "            print(f\"The predicted class is: {predicted_label}, with confidence: {predicted_confidence:.2%}\")\n",
    "\n",
    "        # Sleep for 2 seconds before the next prediction\n",
    "        #time.sleep(2.0)\n",
    "\n",
    "# Call the continuous sound prediction function\n",
    "continuous_sound_prediction(model, device, transformation, SAMPLE_RATE, SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: drilling, with confidence: 93.71%\n",
      "The predicted class is: dog_bark, with confidence: 50.20%\n",
      "The predicted class is: dog_bark, with confidence: 50.61%\n",
      "The predicted class is: drilling, with confidence: 97.13%\n",
      "The predicted class is: drilling, with confidence: 47.26%\n",
      "The predicted class is: drilling, with confidence: 77.08%\n",
      "The predicted class is: drilling, with confidence: 88.79%\n",
      "The predicted class is: dog_bark, with confidence: 60.50%\n",
      "The predicted class is: dog_bark, with confidence: 41.40%\n",
      "The predicted class is: dog_bark, with confidence: 26.91%\n",
      "The predicted class is: dog_bark, with confidence: 98.20%\n",
      "The predicted class is: dog_bark, with confidence: 96.61%\n",
      "The predicted class is: dog_bark, with confidence: 56.29%\n",
      "The predicted class is: dog_bark, with confidence: 40.57%\n",
      "The predicted class is: drilling, with confidence: 99.35%\n",
      "The predicted class is: drilling, with confidence: 99.76%\n",
      "The predicted class is: drilling, with confidence: 51.85%\n",
      "The predicted class is: dog_bark, with confidence: 60.69%\n",
      "The predicted class is: dog_bark, with confidence: 43.18%\n",
      "The predicted class is: drilling, with confidence: 91.22%\n",
      "The predicted class is: dog_bark, with confidence: 53.25%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cafalena/HOUSE/@Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb 셀 6\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W5sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe predicted class is: \u001b[39m\u001b[39m{\u001b[39;00mpredicted_label\u001b[39m}\u001b[39;00m\u001b[39m, with confidence: \u001b[39m\u001b[39m{\u001b[39;00mpredicted_confidence\u001b[39m:\u001b[39;00m\u001b[39m.2%\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         \u001b[39m# Sleep for 2 seconds before the next prediction\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W5sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         \u001b[39m#time.sleep(2.0)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Call the continuous sound prediction function\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m continuous_sound_prediction(model, device, transformation, SAMPLE_RATE, SAMPLE_RATE)\n",
      "\u001b[1;32m/Users/cafalena/HOUSE/@Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb 셀 6\u001b[0m in \u001b[0;36mcontinuous_sound_prediction\u001b[0;34m(model, device, transformation, sample_rate, target_sample_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m duration \u001b[39m=\u001b[39m \u001b[39m2.0\u001b[39m  \u001b[39m# seconds\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m recording \u001b[39m=\u001b[39m sd\u001b[39m.\u001b[39mrec(\u001b[39mint\u001b[39m(duration \u001b[39m*\u001b[39m sample_rate), samplerate\u001b[39m=\u001b[39msample_rate, channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m sd\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Convert to PyTorch tensor and switch channels and frames\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m recording \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(recording)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torchenv/lib/python3.8/site-packages/sounddevice.py:395\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ignore_errors)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for `play()`/`rec()`/`playrec()` to be finished.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[1;32m    381\u001b[0m \u001b[39mPlayback/recording can be stopped with a `KeyboardInterrupt`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m _last_callback:\n\u001b[0;32m--> 395\u001b[0m     \u001b[39mreturn\u001b[39;00m _last_callback\u001b[39m.\u001b[39;49mwait(ignore_errors)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torchenv/lib/python3.8/site-packages/sounddevice.py:2601\u001b[0m, in \u001b[0;36m_CallbackContext.wait\u001b[0;34m(self, ignore_errors)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for finished_callback.\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m \n\u001b[1;32m   2597\u001b[0m \u001b[39mCan be interrupted with a KeyboardInterrupt.\u001b[39;00m\n\u001b[1;32m   2598\u001b[0m \n\u001b[1;32m   2599\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2600\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2601\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevent\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m   2602\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2603\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mclose(ignore_errors)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torchenv/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torchenv/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def continuous_sound_prediction(model, device, transformation, sample_rate, target_sample_rate):\n",
    "    # Define class labels\n",
    "    class_labels = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', \n",
    "                    'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music']\n",
    "    \n",
    "    while True:\n",
    "        # Record a 2 seconds mono audio at the specified sample rate\n",
    "        duration = 2.0  # seconds\n",
    "        recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1) \n",
    "        sd.wait()\n",
    "\n",
    "        # Convert to PyTorch tensor and switch channels and frames\n",
    "        recording = torch.from_numpy(recording).float()\n",
    "        recording = torch.transpose(recording, 0, 1)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if sample_rate != target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, target_sample_rate)\n",
    "            recording = resampler(recording)\n",
    "\n",
    "        # Mix down if necessary\n",
    "        if recording.shape[0] > 1:\n",
    "            recording = torch.mean(recording, dim=0, keepdim=True)\n",
    "\n",
    "        # Cut or pad if necessary\n",
    "        if recording.shape[1] > target_sample_rate:\n",
    "            recording = recording[:, :target_sample_rate]\n",
    "        elif recording.shape[1] < target_sample_rate:\n",
    "            num_missing_samples = target_sample_rate - recording.shape[1]\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            recording = nn.functional.pad(recording, last_dim_padding)\n",
    "\n",
    "        # Apply transformation\n",
    "        recording = transformation(recording)\n",
    "\n",
    "        # Make the prediction\n",
    "        model.eval()  # set model to evaluation mode\n",
    "        with torch.no_grad():  # deactivate autograd engine to reduce memory usage and speed up computations\n",
    "            recording = recording.to(device)\n",
    "            outputs = model(recording[None, ...])\n",
    "            probabilities = F.softmax(outputs, dim=1)  # apply softmax to output\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Get predicted label and its corresponding probability\n",
    "        predicted_label = class_labels[predicted.item()]\n",
    "        predicted_confidence = probabilities[0, predicted.item()].item()  # get the probability of the predicted class\n",
    "\n",
    "        # Only print the output if the confidence is greater than 80% and the label is not in the specified list\n",
    "        if predicted_confidence >= 0.0 and predicted_label not in ['air_conditioner', 'children_playing', 'street_music']:#THE EXCLUDED LABLES\n",
    "            print(f\"The predicted class is: {predicted_label}, with confidence: {predicted_confidence:.2%}\")\n",
    "        # Sleep for 2 seconds before the next prediction\n",
    "        #time.sleep(2.0)\n",
    "\n",
    "# Call the continuous sound prediction function\n",
    "continuous_sound_prediction(model, device, transformation, SAMPLE_RATE, SAMPLE_RATE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
