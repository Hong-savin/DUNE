{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UI4\n",
    "updates\n",
    "1. Better Sound Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mic device pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device #0 name: 갤럭시 S2 마이크\n",
      "Device #1 name: MacBook Air 마이크\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "def list_input_devices():\n",
    "    devices = sd.query_devices()\n",
    "    for i, device in enumerate(devices):\n",
    "        if device['max_input_channels'] > 0:  # this is an input device\n",
    "            print(f\"Device #{i} name: {device['name']}\")\n",
    "\n",
    "# List available input devices (including microphones)\n",
    "list_input_devices()\n",
    "device_id = 2 # <<<  -  pick your mic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mic Test\n",
    "MIC TEST type 1 if you want to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #MIC TEST type 1 if you want to test\n",
    "# test = 1\n",
    "# if test == 1:\n",
    "#     import numpy as np\n",
    "\n",
    "#     # Choose the device to use for recording\n",
    "#     duration = 2  # seconds\n",
    "\n",
    "#     # Create a buffer to store the audio data\n",
    "#     buffer = np.zeros((duration * 44100,))\n",
    "#     buffer_index = 0\n",
    "\n",
    "#     # Define a callback function to process the audio input\n",
    "#     def audio_callback(indata, frames, time, status):\n",
    "#         global buffer_index\n",
    "#         volume_norm = np.linalg.norm(indata) * 10\n",
    "#         print(f'\\r{\"|\" * int(volume_norm)}', end='')  # print a simple \"volume bar\"\n",
    "\n",
    "#         # Store the incoming data in the buffer\n",
    "#         buffer[buffer_index:buffer_index+frames] = indata[:, 0]\n",
    "#         buffer_index += frames\n",
    "\n",
    "#     # Create a stream object\n",
    "#     stream = sd.InputStream(callback=audio_callback, device=device_id, channels=1, samplerate=44100)\n",
    "\n",
    "#     # Start the stream\n",
    "#     with stream:\n",
    "#         # Record for 3 seconds\n",
    "#         sd.sleep(duration * 1000)\n",
    "\n",
    "#     # Play back the recorded sound\n",
    "#     sd.play(buffer, samplerate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick the labels that you want to use here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sound_rate (only raspberrypi use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = '''pcm.!default {\n",
    "#     type asym\n",
    "#     capture.pcm \"mic\"\n",
    "#     playback.pcm \"speaker\"\n",
    "# }\n",
    "# pcm.mic {\n",
    "#     type plug\n",
    "#     slave {\n",
    "#         pcm \"hw:2,0\"\n",
    "#     }\n",
    "# }\n",
    "# pcm.speaker {\n",
    "#     type plug\n",
    "#     slave {\n",
    "#         pcm \"hw:1,0\"\n",
    "#     }\n",
    "# }'''\n",
    "\n",
    "# # 파일 경로와 이름 설정\n",
    "# file_path = \"/home/pi/.asoundrc\"\n",
    "\n",
    "# # 텍스트 파일 생성 및 저장\n",
    "# with open(file_path, \"w\") as file:\n",
    "#     file.write(text)\n",
    "\n",
    "# print(\"Text saved to:\", file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to the model: ../@AI/AI/models/Batch4-1k_004.pth\n"
     ]
    }
   ],
   "source": [
    "# Relative path to the model file\n",
    "path_to_model = '../@AI/AI/models/Batch4-1k_004.pth'\n",
    "print(\"Path to the model:\", path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded deivce :  cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "#One GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(512, 14)\n",
    "try:\n",
    "    state_dict = torch.load(path_to_model, map_location=device)\n",
    "    new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    print(\"Model successfully loaded deivce : \",device)\n",
    "except:\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"Failed to load the model. Please check the model file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from torchvision import transforms  # Import the transforms module\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "# Normalize\n",
    "class MinMaxNormalize(nn.Module):\n",
    "    def __init__(self, min_val=None, max_val=None):\n",
    "        super(MinMaxNormalize, self).__init__()\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        if self.min_val is None or self.max_val is None:\n",
    "            min_val = torch.min(tensor)\n",
    "            max_val = torch.max(tensor)\n",
    "        else:\n",
    "            min_val = self.min_val\n",
    "            max_val = self.max_val\n",
    "        \n",
    "        normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "        return normalized_tensor\n",
    "\n",
    "\n",
    "# Mono To Color (for 3 channels)\n",
    "class MonoToColor(nn.Module):\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(MonoToColor, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor.repeat(self.num_channels, 1, 1)\n",
    "\n",
    "# Transform + appy all func\n",
    "transformation = transforms.Compose([\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=40),# higher the better but more complex. For talking we use 128, for sound effect, about 40.\n",
    "    torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80),\n",
    "    MinMaxNormalize(),\n",
    "    MonoToColor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device #0 name: 갤럭시 S2 마이크\n",
      "Device #1 name: MacBook Air 마이크\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "\n",
    "def list_input_devices():\n",
    "    audio = pyaudio.PyAudio()\n",
    "    for i in range(audio.get_device_count()):\n",
    "        device_info = audio.get_device_info_by_index(i)\n",
    "        if device_info['maxInputChannels'] > 0:  # This is an input device\n",
    "            print(f\"Device #{i} name: {device_info['name']}\")\n",
    "\n",
    "def record_audio():\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=pyaudio.paInt16, channels=1, rate=16000, frames_per_buffer=1024, input=True , input_device_index=device_id)\n",
    "    frames = []\n",
    "    for i in range(0, int(16000 / 1024 * 3)):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "    stream.close()\n",
    "    audio.terminate()  # close the PyAudio object\n",
    "    return b''.join(frames)\n",
    "\n",
    "# def record_audio(seconds=3, samplerate=16000):\n",
    "#     audio_data = sd.rec(int(samplerate * seconds), samplerate=samplerate, channels=1, dtype='int16')\n",
    "#     sd.wait()  # Wait for the recording to finish\n",
    "#     audio_bytes = audio_data.tobytes()\n",
    "#     return audio_bytes\n",
    "\n",
    "\n",
    "def transcribe_audio(audio):\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio_data = sr.AudioData(audio, 16000, 2)\n",
    "    try:\n",
    "        text = recognizer.recognize_sphinx(audio_data)\n",
    "        \n",
    "        return text\n",
    "    except:\n",
    "        return None\n",
    "def main(times):\n",
    "    for i in range(0, times):\n",
    "        audio = record_audio()\n",
    "        text = transcribe_audio(audio)\n",
    "        print(text)\n",
    "TIMES = 0\n",
    "if __name__ == \"__main__\":\n",
    "    main(TIMES)\n",
    "\n",
    "list_input_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOUND EFFECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cpu\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "import sounddevice as sd\n",
    "global count\n",
    "count = 0\n",
    "print(\"device : \",device)\n",
    "## print every labels\n",
    "def continuous_sound_prediction(model, device, transformation, sample_rate):\n",
    "\n",
    "    global count\n",
    "\n",
    "    count = count + 1\n",
    "\n",
    "    labels = [\n",
    "        \"nothing a\", \"nothing b\", \"Car Horn\", \"nothing d\", \"Glass Shatter\", \n",
    "        \"Bell Ring\", \"nothing g\", \"nothing h\", \"nothing i\", \"nothing j\", \n",
    "        \"nothing k\", \"nothing l\", \"nothing m\", \"siren\"\n",
    "    ]\n",
    "\n",
    "    duration = 2.0  # seconds\n",
    "    #recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, device=device_id)\n",
    "    #recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, device=device_id)\n",
    "    #sd.wait()\n",
    "\n",
    "\n",
    "    sample_rate = 44100\n",
    "    duration = 2\n",
    "    input_device = device_id  # Adjust this based on your input device\n",
    "\n",
    "    # Record audio\n",
    "    audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, device=device_id)\n",
    "\n",
    "    # Wait for recording to finish\n",
    "    sd.wait()\n",
    "\n",
    "\n",
    "    ####\n",
    "    \n",
    "    # import sounddevice as sd\n",
    "    # import numpy as np\n",
    "\n",
    "    # sample_rate = 44100\n",
    "    # duration = 10\n",
    "    # input_device = 1  # Adjust this based on your input device\n",
    "\n",
    "    # # Record audio\n",
    "    # audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=2, device=input_device)\n",
    "\n",
    "    # # Wait for recording to finish\n",
    "    # sd.wait()\n",
    "\n",
    "    ####\n",
    "    # Preprocessing\n",
    "    recording = torch.from_numpy(audio_data).float().transpose(0, 1)\n",
    "    if recording.shape[0] > 1:\n",
    "        recording = torch.mean(recording, dim=0, keepdim=True)\n",
    "    recording = nn.functional.pad(recording, (0, sample_rate - recording.shape[1]))\n",
    "    \n",
    "    # Transformation\n",
    "    recording = transformation(recording)\n",
    "    \n",
    "    # Prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        recording = recording.to(device)\n",
    "        outputs = model(recording.unsqueeze(0))\n",
    "        #probabilities = F.softmax(outputs, dim=1)\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "    # Print results\n",
    "    probs = [f\"{label} {prob:.2%}\" for label, prob in zip(labels, probabilities[0])]\n",
    "    print(f\"{count} / {' / '.join(probs)}\")\n",
    "\n",
    "    max_prob, predicted_label_idx = probabilities[0].max(0)\n",
    "    max_prob_label = labels[predicted_label_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if max_prob > 0.0:\n",
    "        if max_prob_label == 'nothing':\n",
    "            return '' , 0 , \"\"\n",
    "        return max_prob_label, max_prob, probabilities\n",
    "    else:\n",
    "        return \"\" , 0 , \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 결과 값 : \n",
      "RPi.GPIO import failed. No vibration\n",
      "pass vibration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.fonts: Populating font family aliases took 170 ms. Replace uses of missing font family \"AppleSystemUIFont\" with one that exists to avoid this cost. \n"
     ]
    },
    {
     "ename": "PortAudioError",
     "evalue": "Error opening InputStream: Invalid number of channels [PaErrorCode -9998]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPortAudioError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/owo/HOUSE/@DUNE/UI/UI4-2.ipynb 셀 19\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/%40DUNE/UI/UI4-2.ipynb#X24sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/%40DUNE/UI/UI4-2.ipynb#X24sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     text_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m#empty the text strings\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/owo/HOUSE/%40DUNE/UI/UI4-2.ipynb#X24sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     predicted_label, predicted_confidence, probabilities \u001b[39m=\u001b[39m continuous_sound_prediction(model, device, transformation, SAMPLE_RATE) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/%40DUNE/UI/UI4-2.ipynb#X24sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/%40DUNE/UI/UI4-2.ipynb#X24sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m/Users/owo/HOUSE/@DUNE/UI/UI4-2.ipynb 셀 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/%40DUNE/UI/UI4-2.ipynb#X24sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m input_device \u001b[39m=\u001b[39m device_id  \u001b[39m# Adjust this based on your input device\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/%40DUNE/UI/UI4-2.ipynb#X24sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Record audio\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/owo/HOUSE/%40DUNE/UI/UI4-2.ipynb#X24sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m audio_data \u001b[39m=\u001b[39m sd\u001b[39m.\u001b[39mrec(\u001b[39mint\u001b[39m(duration \u001b[39m*\u001b[39m sample_rate), samplerate\u001b[39m=\u001b[39msample_rate, channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, device\u001b[39m=\u001b[39mdevice_id)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/%40DUNE/UI/UI4-2.ipynb#X24sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Wait for recording to finish\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/%40DUNE/UI/UI4-2.ipynb#X24sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m sd\u001b[39m.\u001b[39mwait()\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/sounddevice.py:276\u001b[0m, in \u001b[0;36mrec\u001b[0;34m(frames, samplerate, channels, dtype, out, mapping, blocking, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m     ctx\u001b[39m.\u001b[39mread_indata(indata)\n\u001b[1;32m    274\u001b[0m     ctx\u001b[39m.\u001b[39mcallback_exit()\n\u001b[0;32m--> 276\u001b[0m ctx\u001b[39m.\u001b[39mstart_stream(InputStream, samplerate, ctx\u001b[39m.\u001b[39minput_channels,\n\u001b[1;32m    277\u001b[0m                  ctx\u001b[39m.\u001b[39minput_dtype, callback, blocking, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    278\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/sounddevice.py:2582\u001b[0m, in \u001b[0;36m_CallbackContext.start_stream\u001b[0;34m(self, StreamClass, samplerate, channels, dtype, callback, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_stream\u001b[39m(\u001b[39mself\u001b[39m, StreamClass, samplerate, channels, dtype, callback,\n\u001b[1;32m   2580\u001b[0m                  blocking, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2581\u001b[0m     stop()  \u001b[39m# Stop previous playback/recording\u001b[39;00m\n\u001b[0;32m-> 2582\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m StreamClass(samplerate\u001b[39m=\u001b[39msamplerate,\n\u001b[1;32m   2583\u001b[0m                               channels\u001b[39m=\u001b[39mchannels,\n\u001b[1;32m   2584\u001b[0m                               dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   2585\u001b[0m                               callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m   2586\u001b[0m                               finished_callback\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinished_callback,\n\u001b[1;32m   2587\u001b[0m                               \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   2588\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mstart()\n\u001b[1;32m   2589\u001b[0m     \u001b[39mglobal\u001b[39;00m _last_callback\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/sounddevice.py:1421\u001b[0m, in \u001b[0;36mInputStream.__init__\u001b[0;34m(self, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, samplerate\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, blocksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1392\u001b[0m              device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, channels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, latency\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1393\u001b[0m              extra_settings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, finished_callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1394\u001b[0m              clip_off\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither_off\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, never_drop_input\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1395\u001b[0m              prime_output_buffers_using_stream_callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1396\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"PortAudio input stream (using NumPy).\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m \n\u001b[1;32m   1398\u001b[0m \u001b[39m    This has the same methods and attributes as `Stream`, except\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \n\u001b[1;32m   1420\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1421\u001b[0m     _StreamBase\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m, wrap_callback\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39marray\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1422\u001b[0m                          \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_remove_self(\u001b[39mlocals\u001b[39m()))\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/sounddevice.py:898\u001b[0m, in \u001b[0;36m_StreamBase.__init__\u001b[0;34m(self, kind, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback, userdata, wrap_callback)\u001b[0m\n\u001b[1;32m    896\u001b[0m     userdata \u001b[39m=\u001b[39m _ffi\u001b[39m.\u001b[39mNULL\n\u001b[1;32m    897\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ptr \u001b[39m=\u001b[39m _ffi\u001b[39m.\u001b[39mnew(\u001b[39m'\u001b[39m\u001b[39mPaStream**\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 898\u001b[0m _check(_lib\u001b[39m.\u001b[39mPa_OpenStream(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ptr, iparameters, oparameters,\n\u001b[1;32m    899\u001b[0m                           samplerate, blocksize, stream_flags,\n\u001b[1;32m    900\u001b[0m                           callback_ptr, userdata),\n\u001b[1;32m    901\u001b[0m        \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mError opening \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    903\u001b[0m \u001b[39m# dereference PaStream** --> PaStream*\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ptr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ptr[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/sounddevice.py:2747\u001b[0m, in \u001b[0;36m_check\u001b[0;34m(err, msg)\u001b[0m\n\u001b[1;32m   2744\u001b[0m     hosterror_info \u001b[39m=\u001b[39m host_api, info\u001b[39m.\u001b[39merrorCode, hosterror_text\n\u001b[1;32m   2745\u001b[0m     \u001b[39mraise\u001b[39;00m PortAudioError(errormsg, err, hosterror_info)\n\u001b[0;32m-> 2747\u001b[0m \u001b[39mraise\u001b[39;00m PortAudioError(errormsg, err)\n",
      "\u001b[0;31mPortAudioError\u001b[0m: Error opening InputStream: Invalid number of channels [PaErrorCode -9998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode now :   False\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from PyQt5 import QtCore, QtGui, QtWidgets\n",
    "from PyQt5.QtCore import QThread, pyqtSignal\n",
    "from PyQt5.QtGui import QMovie, QIcon\n",
    "from time import sleep\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton, QVBoxLayout, QWidget\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyaudio\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.autograd import Variable\n",
    "from gpiozero import OutputDevice\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "###### INIT ######\n",
    "p = pyaudio.PyAudio()\n",
    "sample_rate = SAMPLE_RATE\n",
    "target_sample_rate = SAMPLE_RATE\n",
    "mode = 1 ## 1 starts with sound effect, 0 starts with stt\n",
    "print(\" 결과 값 : \")\n",
    "\n",
    "\n",
    "# Sound Analysis class running on a separate thread\n",
    "class SoundAnalysis(QThread):\n",
    "    # Define a pyqtSignal with str type, which will be used to send the analysis results to the main thread\n",
    "    result_signal = pyqtSignal(str, float)  # Add a float type for the probability\n",
    "\n",
    "    def __init__(self, model, device, transformation, sample_rate):\n",
    "        QThread.__init__(self)\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.transformation = transformation\n",
    "        self.sample_rate = sample_rate\n",
    "        self.motor_pin=21\n",
    "\n",
    "    def run_motor(self,duration):\n",
    "        motor=OutputDevice(self.motor_pin)\n",
    "        motor.on()\n",
    "        sleep(duration)\n",
    "        motor.off()\n",
    "\n",
    "    def run(self):## Main code\n",
    "        count = 0\n",
    "        text_str = \"\"\n",
    "        while True:\n",
    "\n",
    "            if mode == True:\n",
    "                text_str = \"\" #empty the text strings\n",
    "\n",
    "                predicted_label, predicted_confidence, probabilities = continuous_sound_prediction(model, device, transformation, SAMPLE_RATE) \n",
    "                if mode == True:\n",
    "                    try:\n",
    "                        self.result_signal.emit(predicted_label, predicted_confidence)\n",
    "                        if predicted_confidence > 0.90:\n",
    "                            duration = 3\n",
    "                            self.run_motor(duration)\n",
    "                        #vibration()\n",
    "                    except:\n",
    "                        self.result_signal.emit(predicted_label, predicted_confidence)\n",
    "                    count = count + 1\n",
    "            if mode == False:\n",
    "\n",
    "                times = 2\n",
    "                for _ in range(0, times):\n",
    "                    audio = record_audio()\n",
    "                    text = transcribe_audio(audio)\n",
    "                    print(text)\n",
    "                    text_str += text + \" \"\n",
    "                    if mode == True: ## This is checking again if the button is pushed. If I dont add this, it will reset_label and show the text_str again before going to mode True\n",
    "                        break\n",
    "                    self.result_signal.emit(text_str,0)\n",
    "        \n",
    "    def reset_label(self):# Restting the labels when the button is pressed\n",
    "        self.result_signal.emit(\"켜는중\", 0)\n",
    "        \n",
    "class Ui_MainWindow(object):\n",
    "    def setupUi(self, MainWindow):\n",
    "        MainWindow.setObjectName(\"MainWindow\")\n",
    "        MainWindow.resize(640, 480)\n",
    "        \n",
    "        # 아이콘 파일 경로 설정 (ico 또는 이미지 파일 형식 지원)\n",
    "        icon_path = '../IMAGE/main.png'  # 아이콘 파일 경로를 지정합니다.\n",
    "        # 아이콘을 설정합니다.\n",
    "        MainWindow.setWindowIcon(QIcon(icon_path))  # QIcon 클래스를 사용하여 아이콘을 설정합니다.\n",
    "        \n",
    "        \n",
    "        MainWindow.setFocusPolicy(QtCore.Qt.NoFocus)\n",
    "        MainWindow.setStyleSheet(\"background: white;\")\n",
    "        self.received_text = \"\"\n",
    "        self.centralwidget = QtWidgets.QWidget(MainWindow)\n",
    "        self.centralwidget.setObjectName(\"centralwidget\")\n",
    "        \n",
    "        # Create a QScrollArea\n",
    "        self.scrollArea = QtWidgets.QScrollArea(self.centralwidget)\n",
    "        self.scrollArea.setGeometry(QtCore.QRect(10, 300, 611, 121))\n",
    "        self.scrollArea.setWidgetResizable(True)\n",
    "        self.scrollArea.setObjectName(\"scrollArea\")\n",
    "\n",
    "        # Create the label inside the QScrollArea\n",
    "        self.label = QtWidgets.QLabel(self.scrollArea)\n",
    "        self.label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.label.setStyleSheet(\"background-color: white;\")\n",
    "\n",
    "        self.label.setObjectName(\"text_label\")\n",
    "        self.label.setWordWrap(True)\n",
    "        self.scrollArea.setWidget(self.label)\n",
    "\n",
    "        self.label_2 = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.label_2.setGeometry(QtCore.QRect(10, 10, 370, 261))\n",
    "        self.label_2.setStyleSheet(\"background: transparent;\")\n",
    "\n",
    "        \n",
    "        self.label_2.setText(\"\")\n",
    "        self.label_2.setObjectName(\"imgae_image\")\n",
    "        \n",
    "    \n",
    "        \n",
    "        self.pushButton = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.pushButton.setGeometry(QtCore.QRect(420, 50, 171, 111))\n",
    "        font = QtGui.QFont()\n",
    "        font.setFamily(\"Eras Demi ITC\") #폰트 이름\n",
    "        font.setPointSize(30) #글씨 크기\n",
    "        self.pushButton.setFont(font)\n",
    "        self.pushButton.setObjectName(\"pushButton\")\n",
    "        self.pushButton.setText(\"Switch\")\n",
    "        self.pushButton.setStyleSheet(\"QPushButton {\\n\"\n",
    "\"background: white;\\n\"\n",
    "\"    border-radius: 10px; /* 원하는 모서리 둥글기 정도를 설정 */\\n\"\n",
    "\"    border: 2px solid gray; /* 버튼 테두리 스타일 및 색상 설정 */\\n\"\n",
    "\"}\\n\"\n",
    "\"\")\n",
    "\n",
    "        MainWindow.setCentralWidget(self.centralwidget)\n",
    "\n",
    "        #english_text_label\n",
    "        self.new_label = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.new_label.setGeometry(QtCore.QRect(40, 200, 561, 201))\n",
    "        self.label.setStyleSheet(\"QLabel {\\n\"\n",
    "\"    border: 2px black; /* 테두리 스타일 및 색상 설정 (선택 사항) */\\n\"\n",
    "\"    border-radius: 10px; /* 원하는 모서리 둥글기 정도를 설정 */\\n\"\n",
    "\"    background-color: white; /* 배경색 설정 (선택 사항) */\\n\"\n",
    "\"    padding: 10px; /* 라벨 내부 여백 설정 (선택 사항) */\\n\"\n",
    "\"}\")\n",
    "        self.new_label.setText(\"\")\n",
    "        self.new_label.setAlignment(QtCore.Qt.AlignCenter)  # 텍스트 중앙 정렬\n",
    "        self.new_label.setObjectName(\"english_text_label\")\n",
    "        self.new_label.hide()  # Hide the new label initially\n",
    "        \n",
    "        #english_image_label\n",
    "        self.new_label_2 = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.new_label_2.setGeometry(QtCore.QRect(90, 70, 191, 161))\n",
    "        self.new_label_2.setStyleSheet(\"background: transparent;\")\n",
    "        self.new_label_2.setPixmap(QtGui.QPixmap(\"english_image.png\"))\n",
    "        self.new_label_2.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.new_label_2.setObjectName(\"english_image_label\")\n",
    "        self.new_label_2.hide()\n",
    "\n",
    "        MainWindow.setCentralWidget(self.centralwidget)\n",
    "        self.menubar = QtWidgets.QMenuBar(MainWindow)\n",
    "        self.menubar.setGeometry(QtCore.QRect(0, 0, 800, 29))\n",
    "        self.menubar.setObjectName(\"menubar\")\n",
    "        MainWindow.setMenuBar(self.menubar)\n",
    "        self.statusbar = QtWidgets.QStatusBar(MainWindow)\n",
    "        self.statusbar.setObjectName(\"statusbar\")\n",
    "        MainWindow.setStatusBar(self.statusbar)\n",
    "\n",
    "        self.retranslateUi(MainWindow)\n",
    "        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n",
    "\n",
    "        self.pushButton.clicked.connect(self.onPushButtonClicked)\n",
    "\n",
    "        # Initialize SoundAnalysis and connect the result_signal with the updateLabel function\n",
    "        self.sound_analysis = SoundAnalysis(model, device, transformation, SAMPLE_RATE)\n",
    "        self.sound_analysis.result_signal.connect(self.updateLabel)\n",
    "        self.sound_analysis.result_signal.connect(self.updateLabel2)\n",
    "        self.sound_analysis.start()  # Start the sound analysis thread\n",
    "        \n",
    "        self.is_new_labels_visible = False\n",
    "\n",
    "        \n",
    "\n",
    "    def onPushButtonClicked(self): ######BUTTON#######\n",
    "        global mode\n",
    "        mode = not mode\n",
    "        if mode == 1:\n",
    "            self.new_label_2.hide()  # Hide the new label initially\n",
    "        else:\n",
    "            self.new_label_2.show()\n",
    "        self.sound_analysis.reset_label()#reset the screen (its connected to the sound analysis class)\n",
    "        print(\"mode now :  \",mode)\n",
    "\n",
    "    def updateLabel(self, predicted_label, predicted_confidence):\n",
    "        #print(\"Received signal\")  # Print message when signal is received\n",
    "        if predicted_confidence == 0:\n",
    "            self.label.setText(f\"{predicted_label}\")\n",
    "        else:\n",
    "            self.label.setText(f\"{predicted_label}  {predicted_confidence*100:.2f}%\")\n",
    "        #print(predicted_label)\n",
    "        \n",
    "        # After updating the text, ensure the QScrollArea scrolls to the bottom\n",
    "        self.scrollArea.verticalScrollBar().setValue(self.scrollArea.verticalScrollBar().maximum())\n",
    "\n",
    "        \n",
    "        \n",
    "    def updateLabel2(self, predicted_label):\n",
    "        relative_image_folder_path = \"../IMAGE/GIF\" #\\ only works for windows\n",
    "        image_folder_path= os.path.join(current_path, relative_image_folder_path)\n",
    "        full_file_name = os.path.join(image_folder_path, f\"{predicted_label}.gif\")\n",
    "        self.movie = QMovie(full_file_name)\n",
    "        self.label_2.setMovie(self.movie)\n",
    "        self.movie.start()\n",
    "        #print(predicted_label)\n",
    "\n",
    "\n",
    "    def retranslateUi(self, MainWindow):\n",
    "        _translate = QtCore.QCoreApplication.translate\n",
    "        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"Sound\"))\n",
    "        self.label.setFont(QtGui.QFont(\"AppleSystemUIFont\",20))\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "       \n",
    "        \n",
    "try:\n",
    "    import RPi.GPIO as GPIO\n",
    "    print(\"RPi.GPIO loaded successfully\")\n",
    "    import threading\n",
    "    print(\"threading loaded successfully\")\n",
    "\n",
    "    class VibrationController:\n",
    "        def __init__(self):\n",
    "            # 핀 번호 설정\n",
    "            self.red_button_pin = 5\n",
    "            self.yellow_button_pin = 6\n",
    "            self.green_button_pin = 13\n",
    "            self.vibration_motor_pin = 12\n",
    "            \n",
    "            # 진동 세기 초기화\n",
    "            self.vibration_intensity_temp = 100\n",
    "            self.vibration_intensity = 0\n",
    "            \n",
    "            # 이전 스위치 상태 초기화\n",
    "            self.prev_red_button_state = GPIO.HIGH\n",
    "            self.prev_yellow_button_state = GPIO.HIGH\n",
    "            self.prev_green_button_state = GPIO.HIGH\n",
    "            \n",
    "            self.debounce_time = 0.2\n",
    "            \n",
    "            # 진동 상태를 저장하는 변수\n",
    "            self.vibration_on = False\n",
    "            \n",
    "            # GPIO 초기화\n",
    "            GPIO.setmode(GPIO.BCM)\n",
    "            GPIO.setup(self.red_button_pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "            GPIO.setup(self.yellow_button_pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "            GPIO.setup(self.green_button_pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "            GPIO.setup(self.vibration_motor_pin, GPIO.OUT)\n",
    "            \n",
    "            # PWM 설정\n",
    "            self.pwm_frequency = 1000\n",
    "            self.pwm = GPIO.PWM(self.vibration_motor_pin, self.pwm_frequency)\n",
    "            self.pwm.start(self.vibration_intensity)\n",
    "        \n",
    "        def display_vibration_intensity(self):\n",
    "            print(f\"진동 세기: {self.vibration_intensity}\")\n",
    "        \n",
    "        def adjust_vibration_intensity(self, button_pin):\n",
    "            if button_pin == self.red_button_pin:\n",
    "                self.vibration_intensity = self.vibration_intensity_temp\n",
    "            elif button_pin == self.yellow_button_pin:\n",
    "                self.vibration_intensity = max(self.vibration_intensity - 10, 0)\n",
    "            elif button_pin == self.green_button_pin:\n",
    "                self.vibration_intensity = min(self.vibration_intensity + 10, 100)\n",
    "            \n",
    "            self.pwm.ChangeDutyCycle(self.vibration_intensity)\n",
    "            self.display_vibration_intensity()\n",
    "        \n",
    "        def run(self):\n",
    "            try:\n",
    "                while True:\n",
    "                    red_button_state = GPIO.input(self.red_button_pin)\n",
    "                    yellow_button_state = GPIO.input(self.yellow_button_pin)\n",
    "                    green_button_state = GPIO.input(self.green_button_pin)\n",
    "            \n",
    "                    if red_button_state != self.prev_red_button_state:\n",
    "                        time.sleep(self.debounce_time)\n",
    "                        if red_button_state != GPIO.input(self.red_button_pin):\n",
    "                            \n",
    "                            if not self.vibration_on:\n",
    "                                self.vibration_on = True\n",
    "                                self.adjust_vibration_intensity(self.red_button_pin)\n",
    "                            else:\n",
    "                                self.vibration_intensity_temp = self.vibration_intensity\n",
    "                                self.vibration_intensity = 0\n",
    "                                self.pwm.ChangeDutyCycle(self.vibration_intensity)\n",
    "                                self.display_vibration_intensity()\n",
    "                                self.vibration_on = False\n",
    "            \n",
    "                        self.prev_red_button_state = red_button_state\n",
    "            \n",
    "                    if yellow_button_state != self.prev_yellow_button_state:\n",
    "                        time.sleep(self.debounce_time)\n",
    "                        if yellow_button_state != GPIO.input(self.yellow_button_pin):\n",
    "                            self.adjust_vibration_intensity(self.yellow_button_pin)\n",
    "                        self.prev_yellow_button_state = yellow_button_state\n",
    "            \n",
    "                    if green_button_state != self.prev_green_button_state:\n",
    "                        time.sleep(self.debounce_time)\n",
    "                        if green_button_state != GPIO.input(self.green_button_pin):\n",
    "                            self.adjust_vibration_intensity(self.green_button_pin)\n",
    "                        self.prev_green_button_state = green_button_state\n",
    "            \n",
    "                    time.sleep(0.01)  \n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                self.pwm.stop()\n",
    "                GPIO.cleanup()\n",
    "except:\n",
    "    print(\"RPi.GPIO import failed. No vibration\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    MainWindow = QtWidgets.QMainWindow()\n",
    "    try:\n",
    "        vibration_controller = VibrationController()\n",
    "        vibration_thread = threading.Thread(target=vibration_controller.run)\n",
    "        vibration_thread.start()\n",
    "    except:\n",
    "        print(\"pass vibration\")\n",
    "    ui = Ui_MainWindow()\n",
    "    ui.setupUi(MainWindow)\n",
    "    MainWindow.show()\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가능한 폰트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.Apple Color Emoji UI', '.Apple Symbols Fallback', '.AppleHongKongChineseFont', '.AppleJapaneseFont', '.AppleKoreanFont', '.AppleSimplifiedChineseFont', '.AppleSystemFallback', '.AppleSystemUIFont', '.AppleTraditionalChineseFont', '.Arial Hebrew Desk Interface', '.Lucida Grande UI', '.Noto Nastaliq Urdu UI', '.SF Arabic', '.ThonburiUI', 'Academy Engraved LET', 'Al Bayan', 'Al Nile', 'Al Tarikh', 'American Typewriter', 'Andale Mono', 'Apple Braille', 'Apple Chancery', 'Apple Color Emoji', 'Apple LiGothic', 'Apple LiSung', 'Apple SD Gothic Neo', 'Apple Symbols', 'AppleGothic', 'AppleMyungjo', 'Arial', 'Arial Black', 'Arial Hebrew', 'Arial Hebrew Scholar', 'Arial Narrow', 'Arial Rounded MT Bold', 'Arial Unicode MS', 'Avenir', 'Avenir Next', 'Avenir Next Condensed', 'Ayuthaya', 'Baghdad', 'Bai Jamjuree', 'Bangla MN', 'Bangla Sangam MN', 'Baoli SC', 'Baoli TC', 'Baskerville', 'Beirut', 'BiauKaiHK', 'BiauKaiTC', 'Big Caslon', 'BM Dohyeon', 'BM Hanna 11yrs Old', 'BM Hanna Air', 'BM Hanna Pro', 'BM Jua', 'BM Kirang Haerang', 'BM Yeonsung', 'Bodoni 72', 'Bodoni 72 Oldstyle', 'Bodoni 72 Smallcaps', 'Bodoni Ornaments', 'Bradley Hand', 'Brush Script MT', 'Chakra Petch', 'Chalkboard', 'Chalkboard SE', 'Chalkduster', 'Charm', 'Charmonman', 'Charter', 'Cochin', 'Comic Sans MS', 'Copperplate', 'Corsiva Hebrew', 'Courier New', 'Damascus', 'DecoType Naskh', 'Devanagari MT', 'Devanagari Sangam MN', 'Didot', 'DIN Alternate', 'DIN Condensed', 'Diwan Kufi', 'Diwan Thuluth', 'Euphemia UCAS', 'Fahkwang', 'Farah', 'Farisi', 'Futura', 'Galvji', 'GB18030 Bitmap', 'Geeza Pro', 'Geneva', 'Georgia', 'Gill Sans', 'Grantha Sangam MN', 'Gujarati MT', 'Gujarati Sangam MN', 'GungSeo', 'Gurmukhi MN', 'Gurmukhi MT', 'Gurmukhi Sangam MN', 'Hannotate SC', 'Hannotate TC', 'HanziPen SC', 'HanziPen TC', 'HeadLineA', 'Hei', 'Heiti SC', 'Heiti TC', 'Helvetica', 'Helvetica Neue', 'Herculanum', 'Hiragino Maru Gothic ProN', 'Hiragino Mincho ProN', 'Hiragino Sans', 'Hiragino Sans CNS', 'Hiragino Sans GB', 'Hoefler Text', 'Impact', 'InaiMathi', 'ITF Devanagari', 'ITF Devanagari Marathi', 'K2D', 'Kai', 'Kailasa', 'Kaiti SC', 'Kaiti TC', 'Kannada MN', 'Kannada Sangam MN', 'Kefa', 'Khmer MN', 'Khmer Sangam MN', 'Klee', 'Kodchasan', 'Kohinoor Bangla', 'Kohinoor Devanagari', 'Kohinoor Gujarati', 'Kohinoor Telugu', 'KoHo', 'Kokonor', 'Krub', 'Krungthep', 'KufiStandardGK', 'Lantinghei SC', 'Lantinghei TC', 'Lao MN', 'Lao Sangam MN', 'Libian SC', 'Libian TC', 'LiHei Pro', 'LingWai SC', 'LingWai TC', 'LiSong Pro', 'Lucida Grande', 'Luminari', 'Malayalam MN', 'Malayalam Sangam MN', 'Mali', 'Marker Felt', 'Menlo', 'Microsoft Sans Serif', 'Mishafi', 'Mishafi Gold', 'Monaco', 'Mshtakan', 'Mukta Mahee', 'Muna', 'Myanmar MN', 'Myanmar Sangam MN', 'Nadeem', 'Nanum Brush Script', 'Nanum Gothic', 'Nanum Myeongjo', 'Nanum Pen Script', 'New Peninim MT', 'Niramit', 'Noteworthy', 'Noto Nastaliq Urdu', 'Noto Sans Armenian', 'Noto Sans Kannada', 'Noto Sans Myanmar', 'Noto Sans Oriya', 'Noto Sans Zawgyi', 'Noto Serif Myanmar', 'Optima', 'Oriya MN', 'Oriya Sangam MN', 'Osaka', 'Palatino', 'Papyrus', 'Party LET', 'PCMyungjo', 'Phosphate', 'PilGi', 'PingFang HK', 'PingFang SC', 'PingFang TC', 'Plantagenet Cherokee', 'PSL Ornanong Pro', 'PT Mono', 'PT Sans', 'PT Sans Caption', 'PT Sans Narrow', 'PT Serif', 'PT Serif Caption', 'Raanana', 'Rockwell', 'Sana', 'Sarabun', 'Sathu', 'Savoye LET', 'Shree Devanagari 714', 'SignPainter', 'Silom', 'SimSong', 'Sinhala MN', 'Sinhala Sangam MN', 'Skia', 'Snell Roundhand', 'Songti SC', 'Songti TC', 'Srisakdi', 'STFangsong', 'STHeiti', 'STIX Two Math', 'STIX Two Text', 'STKaiti', 'STSong', 'Sukhumvit Set', 'Symbol', 'Tahoma', 'Tamil MN', 'Tamil Sangam MN', 'Telugu MN', 'Telugu Sangam MN', 'Thonburi', 'Times New Roman', 'Toppan Bunkyu Gothic', 'Toppan Bunkyu Midashi Gothic', 'Toppan Bunkyu Midashi Mincho', 'Toppan Bunkyu Mincho', 'Trattatello', 'Trebuchet MS', 'Tsukushi A Round Gothic', 'Tsukushi B Round Gothic', 'Verdana', 'Waseem', 'Wawati SC', 'Wawati TC', 'Webdings', 'Weibei SC', 'Weibei TC', 'Wingdings', 'Wingdings 2', 'Wingdings 3', 'Xingkai SC', 'Xingkai TC', 'Yuanti SC', 'Yuanti TC', 'YuGothic', 'YuKyokasho', 'YuKyokasho Yoko', 'YuMincho', 'YuMincho +36p Kana', 'Yuppy SC', 'Yuppy TC', 'Zapf Dingbats', 'Zapfino']\n"
     ]
    }
   ],
   "source": [
    "from PyQt5.QtGui import QFontDatabase\n",
    "\n",
    "print(QFontDatabase().families())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
