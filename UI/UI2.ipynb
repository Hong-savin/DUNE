{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UI2\n",
    "updates\n",
    "1. pick labels\n",
    "2. change prob acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick the labels that you want to use here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = [\"door_nock\",\"glass_shatter\",\"car_horn\",\"dog_bark\",\"drilling\",\"nothing\",\"siren\",\"nothing2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 경로: c:\\Users\\homey\\Documents\\GitHub\\DUNE\\UI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = os.getcwd()\n",
    "print(\"현재 경로:\", current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to the model: c:\\Users\\homey\\Documents\\GitHub\\DUNE\\UI\\../@AI/Sound Classification/ResNet18_02.pth\n"
     ]
    }
   ],
   "source": [
    "# Relative path to the model file\n",
    "relative_path_to_model = \"../@AI/Sound Classification/ResNet18_02.pth\"\n",
    "\n",
    "# Combine the current path and the relative path to create the absolute path to the model\n",
    "path_to_model = os.path.join(current_path, relative_path_to_model)\n",
    "\n",
    "print(\"Path to the model:\", path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded deivce :  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\homey\\anaconda3\\envs\\homeyyj\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\homey\\anaconda3\\envs\\homeyyj\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "#One GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(512, len(selected_labels))\n",
    "try:\n",
    "    state_dict = torch.load(path_to_model, map_location=device)\n",
    "    new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    print(\"Model successfully loaded deivce : \",device)\n",
    "except:\n",
    "    print(\"Failed to load the model. Please check the model file.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\homey\\anaconda3\\envs\\homeyyj\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "c:\\Users\\homey\\anaconda3\\envs\\homeyyj\\lib\\site-packages\\torchaudio\\functional\\functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from torchvision import transforms  # Import the transforms module\n",
    "\n",
    "\n",
    "#Transform\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "class MonoToColor(nn.Module):\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(MonoToColor, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor.repeat(self.num_channels, 1, 1)\n",
    "\n",
    "# Apply the same transformation as used during training\n",
    "transformation = transforms.Compose([\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=128),\n",
    "    torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80),\n",
    "    MonoToColor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import speech_recognition as sr\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read audio func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio():\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=pyaudio.paInt16, channels=1, rate=16000, frames_per_buffer=1024, input=True)\n",
    "    frames = []\n",
    "    for i in range(0, int(16000 / 1024 * 3)):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "    stream.close()\n",
    "    audio.terminate()  # close the PyAudio object\n",
    "    return b''.join(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio):\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio_data = sr.AudioData(audio, 16000, 2)\n",
    "    try:\n",
    "        text = recognizer.recognize_sphinx(audio_data)\n",
    "        \n",
    "        return text\n",
    "    except:\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spinx Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(times):\n",
    "    for i in range(0, times):\n",
    "        audio = record_audio()\n",
    "        text = transcribe_audio(audio)\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m TIMES \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m     main(TIMES)\n",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(times)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m(times):\n\u001b[0;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, times):\n\u001b[1;32m----> 3\u001b[0m         audio \u001b[39m=\u001b[39m record_audio()\n\u001b[0;32m      4\u001b[0m         text \u001b[39m=\u001b[39m transcribe_audio(audio)\n\u001b[0;32m      5\u001b[0m         \u001b[39mprint\u001b[39m(text)\n",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m, in \u001b[0;36mrecord_audio\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m frames \u001b[39m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mint\u001b[39m(\u001b[39m16000\u001b[39m \u001b[39m/\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m)):\n\u001b[1;32m----> 6\u001b[0m     data \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39;49mread(\u001b[39m1024\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m     frames\u001b[39m.\u001b[39mappend(data)\n\u001b[0;32m      8\u001b[0m stream\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\homey\\anaconda3\\envs\\homeyyj\\lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNot input stream\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39;49mread_stream(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream, num_frames,\n\u001b[0;32m    571\u001b[0m                       exception_on_overflow)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TIMES = 2\n",
    "if __name__ == \"__main__\":\n",
    "    main(TIMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyQt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cpu\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "import sounddevice as sd\n",
    "\n",
    "print(\"device : \",device)\n",
    "## print every labels\n",
    "def continuous_sound_prediction(model, device, transformation, sample_rate, target_sample_rate):\n",
    "    # Define class labels\n",
    "\n",
    "    # Record a 2 seconds mono audio at the specified sample rate\n",
    "    duration = 2.0  # seconds\n",
    "    recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1) \n",
    "    sd.wait()\n",
    "\n",
    "    # Convert to PyTorch tensor and switch channels and frames\n",
    "    recording = torch.from_numpy(recording).float()\n",
    "    recording = torch.transpose(recording, 0, 1)\n",
    "\n",
    "    # Resample if necessary\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, target_sample_rate)\n",
    "        recording = resampler(recording)\n",
    "\n",
    "    # Mix down if necessary\n",
    "    if recording.shape[0] > 1:\n",
    "        recording = torch.mean(recording, dim=0, keepdim=True)\n",
    "\n",
    "    # Cut or pad if necessary\n",
    "    if recording.shape[1] > target_sample_rate:\n",
    "        recording = recording[:, :target_sample_rate]\n",
    "    elif recording.shape[1] < target_sample_rate:\n",
    "        num_missing_samples = target_sample_rate - recording.shape[1]\n",
    "        last_dim_padding = (0, num_missing_samples)\n",
    "        recording = nn.functional.pad(recording, last_dim_padding)\n",
    "\n",
    "    # Apply transformation\n",
    "    recording = transformation(recording)\n",
    "\n",
    "    # Make the prediction\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():  # deactivate autograd engine to reduce memory usage and speed up computations\n",
    "        recording = recording.to(device)\n",
    "        outputs = model(recording[None, ...])\n",
    "        #probabilities = F.softmax(outputs, dim=1)  # apply softmax to output (for 100%)\n",
    "        #_, predicted = torch.max(outputs, 1)\n",
    "        probabilities = torch.sigmoid(outputs)  # apply sigmoid to output (for indivisual points)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Get predicted label and its corresponding probability\n",
    "    predicted_label = selected_labels[predicted.item()]\n",
    "    predicted_confidence = probabilities[0, predicted.item()].item()  # get the probability of the predicted class\n",
    "\n",
    "    ######## Adjust 'x' probability   #########\n",
    "    change_label = \"glass_shatter\"\n",
    "    change_probability = 0.5\n",
    "    try:# if you have something to reduce\n",
    "        x_index = selected_labels.index(change_label)\n",
    "        probabilities[0, x_index] = max(0.0, probabilities[0, x_index].item() - change_probability)\n",
    "    except:#if you dont\n",
    "        pass\n",
    "    # Print the probabilities of all labels in one line\n",
    "    #prob_strs = [f\"{label} {probabilities[0, idx].item():.2%}\" for idx, label in enumerate(selected_labels)]\n",
    "    #print(f\"/ \".join(prob_strs))\n",
    "\n",
    "    if predicted_confidence > 0.95:\n",
    "        return predicted_label, predicted_confidence, probabilities\n",
    "    else:\n",
    "        return \"NONE\" , 0 , \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 결과 값 : \n",
      "pass vibration\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'times'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 283\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpass vibration\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m ui \u001b[39m=\u001b[39m Ui_MainWindow()\n\u001b[1;32m--> 283\u001b[0m ui\u001b[39m.\u001b[39;49msetupUi(MainWindow)\n\u001b[0;32m    284\u001b[0m MainWindow\u001b[39m.\u001b[39mshow()\n\u001b[0;32m    285\u001b[0m sys\u001b[39m.\u001b[39mexit(app\u001b[39m.\u001b[39mexec_())\n",
      "Cell \u001b[1;32mIn[12], line 122\u001b[0m, in \u001b[0;36mUi_MainWindow.setupUi\u001b[1;34m(self, MainWindow)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_new_labels_visible \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39m# Initialize SpeechToText\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstt \u001b[39m=\u001b[39m SpeechToText()\n\u001b[0;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstt\u001b[39m.\u001b[39mresult_signal\u001b[39m.\u001b[39mconnect(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdateEnglishLabel)\n\u001b[0;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msound_analysis\u001b[39m.\u001b[39mstart()\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'times'"
     ]
    }
   ],
   "source": [
    "from PyQt5 import QtCore, QtGui, QtWidgets\n",
    "from PyQt5.QtCore import QThread, pyqtSignal\n",
    "from PyQt5.QtGui import QMovie\n",
    "\n",
    "from time import sleep\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyaudio\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.autograd import Variable\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "sample_rate = SAMPLE_RATE\n",
    "target_sample_rate = SAMPLE_RATE\n",
    "\n",
    "\n",
    "print(\" 결과 값 : \")\n",
    "\n",
    "# Sound Analysis class running on a separate thread\n",
    "class SoundAnalysis(QThread):\n",
    "    # Define a pyqtSignal with str type, which will be used to send the analysis results to the main thread\n",
    "    result_signal = pyqtSignal(str, float)  # Add a float type for the probability\n",
    "\n",
    "    def __init__(self, model, device, transformation, sample_rate):\n",
    "        QThread.__init__(self)\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.transformation = transformation\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def run(self):\n",
    "        count = 0\n",
    "        while True:\n",
    "            predicted_label, predicted_confidence, probabilities = continuous_sound_prediction(model, device, transformation, SAMPLE_RATE, SAMPLE_RATE) \n",
    "            try:               \n",
    "                self.result_signal.emit(predicted_label, predicted_confidence)\n",
    "                prob_strs = [f\"{label} {probabilities[0, idx].item():.2%}\" for idx, label in enumerate(selected_labels)]\n",
    "                #vibration()\n",
    "                print(f\"\\r{count} / \" + \" / \".join(prob_strs), end=\"\")\n",
    "            except:\n",
    "                self.result_signal.emit(predicted_label, predicted_confidence)\n",
    "            count = count + 1\n",
    "\n",
    "class SpeechToText(QThread):\n",
    "    \n",
    "    def __init__(self,times):\n",
    "        QThread.__init__(self)\n",
    "        self.times\n",
    "    def run(self):\n",
    "        for i in range(0, times):\n",
    "            audio = record_audio()\n",
    "            text = transcribe_audio(audio)\n",
    "\n",
    "\n",
    "class Ui_MainWindow(object):\n",
    "    def setupUi(self, MainWindow):\n",
    "        MainWindow.setObjectName(\"MainWindow\")\n",
    "        MainWindow.resize(640, 480)\n",
    "        self.received_text = \"\"\n",
    "        self.centralwidget = QtWidgets.QWidget(MainWindow)\n",
    "        self.centralwidget.setObjectName(\"centralwidget\")\n",
    "        self.label = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.label.setGeometry(QtCore.QRect(40, 300, 561, 91))\n",
    "        self.label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.label.setObjectName(\"text_label\")\n",
    "\n",
    "        self.label_2 = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.label_2.setGeometry(QtCore.QRect(40, 30, 381, 211))\n",
    "        self.label_2.setText(\"\")\n",
    "        self.label_2.setObjectName(\"imgae_image\")\n",
    "\n",
    "        self.pushButton = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.pushButton.setGeometry(QtCore.QRect(470, 40, 141, 101))\n",
    "        self.pushButton.setObjectName(\"pushButton\")\n",
    "        MainWindow.setCentralWidget(self.centralwidget)\n",
    "\n",
    "        #english_text_label\n",
    "        self.new_label = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.new_label.setGeometry(QtCore.QRect(40, 200, 561, 201))\n",
    "        self.new_label.setText(\"\")\n",
    "        self.new_label.setAlignment(QtCore.Qt.AlignCenter)#텍스트 중앙 정렬\n",
    "        self.new_label.setObjectName(\"english_text_label\")\n",
    "        self.new_label.hide()  # Hide the new label initially\n",
    "        #english_image_label\n",
    "        self.new_label_2 = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.new_label_2.setGeometry(QtCore.QRect(50, 40, 144, 130))\n",
    "        self.new_label_2.setPixmap(QtGui.QPixmap(\"english_image.png\"))\n",
    "        self.new_label_2.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.new_label_2.setObjectName(\"english_image_label\")\n",
    "        self.new_label_2.hide()  # Hide the new label initially\n",
    "\n",
    "       \n",
    "\n",
    "       \n",
    "        \n",
    "        MainWindow.setCentralWidget(self.centralwidget)\n",
    "        self.menubar = QtWidgets.QMenuBar(MainWindow)\n",
    "        self.menubar.setGeometry(QtCore.QRect(0, 0, 800, 29))\n",
    "        self.menubar.setObjectName(\"menubar\")\n",
    "        MainWindow.setMenuBar(self.menubar)\n",
    "        self.statusbar = QtWidgets.QStatusBar(MainWindow)\n",
    "        self.statusbar.setObjectName(\"statusbar\")\n",
    "        MainWindow.setStatusBar(self.statusbar)\n",
    "\n",
    "        self.retranslateUi(MainWindow)\n",
    "        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n",
    "\n",
    "        self.pushButton.clicked.connect(self.onPushButtonClicked)\n",
    "\n",
    "        # Initialize SoundAnalysis and connect the result_signal with the updateLabel function\n",
    "        self.sound_analysis = SoundAnalysis(model, device, transformation, SAMPLE_RATE)\n",
    "        self.sound_analysis.result_signal.connect(self.updateLabel)\n",
    "        self.sound_analysis.result_signal.connect(self.updateLabel2)\n",
    "        self.sound_analysis.start()  # Start the sound analysis thread\n",
    "        \n",
    "        self.is_new_labels_visible = False\n",
    "\n",
    "        # Initialize SpeechToText\n",
    "        self.stt = SpeechToText()\n",
    "        self.stt.result_signal.connect(self.updateEnglishLabel)\n",
    "        self.sound_analysis.start()\n",
    "        \n",
    "\n",
    "    def onPushButtonClicked(self):\n",
    "        if self.is_new_labels_visible:\n",
    "            # Hide the new labels\n",
    "            self.new_label.hide()\n",
    "            self.new_label_2.hide()\n",
    "\n",
    "            # Show the original labels\n",
    "            self.label.show()\n",
    "            self.label_2.show()\n",
    "\n",
    "            self.is_new_labels_visible = False\n",
    "        else:\n",
    "            # Hide the original labels\n",
    "            self.label.hide()\n",
    "            self.label_2.hide()\n",
    "\n",
    "            # Show the new labels\n",
    "            self.new_label.show()\n",
    "            self.new_label_2.show()\n",
    "\n",
    "            self.is_new_labels_visible = True\n",
    "        \n",
    "        \n",
    "    def updateLabel2(self, predicted_label):\n",
    "        relative_image_folder_path = \"../png\" #\\ only works for windows\n",
    "        image_folder_path= os.path.join(current_path, relative_image_folder_path)\n",
    "        full_file_name = os.path.join(image_folder_path, f\"{predicted_label}.png\")\n",
    "        #self.movie = QMovie(full_file_name)\n",
    "        #self.label.setMovie(self.movie)\n",
    "        #self.movie.start()\n",
    "        self.label_2.setPixmap(QtGui.QPixmap(full_file_name)) \n",
    "        #print(predicted_label)\n",
    "\n",
    "    def updateLabel(self, predicted_label, predicted_confidence):\n",
    "        #print(\"Received signal\")  # Print message when signal is received\n",
    "        self.label.setText(f\"{predicted_label}  {predicted_confidence*100:.2f}%\")\n",
    "        #print(predicted_label)\n",
    "    \n",
    "    def updateEnglishLabel(self, text):\n",
    "        print(text)\n",
    "        self.new_label.setText(text)\n",
    "\n",
    "\n",
    "    def retranslateUi(self, MainWindow):\n",
    "        _translate = QtCore.QCoreApplication.translate\n",
    "        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"MainWindow\"))\n",
    "        self.label.setFont(QtGui.QFont(\"AppleSystemUIFont\",20))\n",
    "        self.label.setStyleSheet(\"Color : black\")\n",
    "        self.pushButton.setText(_translate(\"MainWindow\", \"PushButton\"))\n",
    "        \n",
    "try:\n",
    "    import RPi.GPIO as GPIO\n",
    "    import threading\n",
    "\n",
    "    class VibrationController:\n",
    "        def __init__(self):\n",
    "            # 핀 번호 설정\n",
    "            self.red_button_pin = 17\n",
    "            self.yellow_button_pin = 22\n",
    "            self.green_button_pin = 27\n",
    "            self.vibration_motor_pin = 18\n",
    "            \n",
    "            # 진동 세기 초기화\n",
    "            self.vibration_intensity_temp = 100\n",
    "            self.vibration_intensity = 0\n",
    "            \n",
    "            # 이전 스위치 상태 초기화\n",
    "            self.prev_red_button_state = GPIO.HIGH\n",
    "            self.prev_yellow_button_state = GPIO.HIGH\n",
    "            self.prev_green_button_state = GPIO.HIGH\n",
    "            \n",
    "            self.debounce_time = 0.2\n",
    "            \n",
    "            # 진동 상태를 저장하는 변수\n",
    "            self.vibration_on = False\n",
    "            \n",
    "            # GPIO 초기화\n",
    "            GPIO.setmode(GPIO.BCM)\n",
    "            GPIO.setup(self.red_button_pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "            GPIO.setup(self.yellow_button_pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "            GPIO.setup(self.green_button_pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n",
    "            GPIO.setup(self.vibration_motor_pin, GPIO.OUT)\n",
    "            \n",
    "            # PWM 설정\n",
    "            self.pwm_frequency = 1000\n",
    "            self.pwm = GPIO.PWM(self.vibration_motor_pin, self.pwm_frequency)\n",
    "            self.pwm.start(self.vibration_intensity)\n",
    "        \n",
    "        def display_vibration_intensity(self):\n",
    "            print(f\"진동 세기: {self.vibration_intensity}\")\n",
    "        \n",
    "        def adjust_vibration_intensity(self, button_pin):\n",
    "            if button_pin == self.red_button_pin:\n",
    "                self.vibration_intensity = self.vibration_intensity_temp\n",
    "            elif button_pin == self.yellow_button_pin:\n",
    "                self.vibration_intensity = max(self.vibration_intensity - 10, 0)\n",
    "            elif button_pin == self.green_button_pin:\n",
    "                self.vibration_intensity = min(self.vibration_intensity + 10, 100)\n",
    "            \n",
    "            self.pwm.ChangeDutyCycle(self.vibration_intensity)\n",
    "            self.display_vibration_intensity()\n",
    "        \n",
    "        def run(self):\n",
    "            try:\n",
    "                while True:\n",
    "                    red_button_state = GPIO.input(self.red_button_pin)\n",
    "                    yellow_button_state = GPIO.input(self.yellow_button_pin)\n",
    "                    green_button_state = GPIO.input(self.green_button_pin)\n",
    "            \n",
    "                    if red_button_state != self.prev_red_button_state:\n",
    "                        time.sleep(self.debounce_time)\n",
    "                        if red_button_state != GPIO.input(self.red_button_pin):\n",
    "                            \n",
    "                            if not self.vibration_on:\n",
    "                                self.vibration_on = True\n",
    "                                self.adjust_vibration_intensity(self.red_button_pin)\n",
    "                            else:\n",
    "                                self.vibration_intensity_temp = self.vibration_intensity\n",
    "                                self.vibration_intensity = 0\n",
    "                                self.pwm.ChangeDutyCycle(self.vibration_intensity)\n",
    "                                self.display_vibration_intensity()\n",
    "                                self.vibration_on = False\n",
    "            \n",
    "                        self.prev_red_button_state = red_button_state\n",
    "            \n",
    "                    if yellow_button_state != self.prev_yellow_button_state:\n",
    "                        time.sleep(self.debounce_time)\n",
    "                        if yellow_button_state != GPIO.input(self.yellow_button_pin):\n",
    "                            self.adjust_vibration_intensity(self.yellow_button_pin)\n",
    "                        self.prev_yellow_button_state = yellow_button_state\n",
    "            \n",
    "                    if green_button_state != self.prev_green_button_state:\n",
    "                        time.sleep(self.debounce_time)\n",
    "                        if green_button_state != GPIO.input(self.green_button_pin):\n",
    "                            self.adjust_vibration_intensity(self.green_button_pin)\n",
    "                        self.prev_green_button_state = green_button_state\n",
    "            \n",
    "                    time.sleep(0.01)  \n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                self.pwm.stop()\n",
    "                GPIO.cleanup()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    MainWindow = QtWidgets.QMainWindow()\n",
    "    try:\n",
    "        vibration_controller = VibrationController()\n",
    "        vibration_thread = threading.Thread(target=vibration_controller.run)\n",
    "        vibration_thread.start()\n",
    "    except:\n",
    "        print(\"pass vibration\")\n",
    "    ui = Ui_MainWindow()\n",
    "    ui.setupUi(MainWindow)\n",
    "    MainWindow.show()\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가능한 폰트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5.QtGui import QFontDatabase\n",
    "\n",
    "print(QFontDatabase().families())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
